%
% File naaclhlt2015.tex
%


\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}

\usepackage{tikz-qtree}
\usepackage{tikz-dependency}
\usepackage{proof}
\usepackage{fullpage}
\usepackage{todonotes}
\usepackage{multicol}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}


\usepackage{amsfonts}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{algpseudocode}
\usepackage{algorithm}

\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
\tikzset{
  >=latex,text height=1.5ex,text depth=0.25ex
}

\newcommand{\nonterms}{\mathcal{N}}
\newcommand{\END}{\mathrm{END}}
\newcommand{\START}{\mathrm{START}}
\newcommand{\rules}{\mathcal{R}}
\newcommand{\terms}{\mathcal{T}}
\newcommand{\Left}[1]{#1_{\Leftarrow}}
\newcommand{\Right}[1]{#1_{\Rightarrow}}
\newcommand{\Span}[1]{\langle #1 \rangle}
% \newcommand{\root}{r}

\newcommand{\Tag}[1]{\texttt{#1}}
\newcommand{\Root}{r}

\newcommand{\RuleSym}{\mathrm{rule}}
\newcommand{\Rule}[3]{#1 \rightarrow #2\ #3}
\newcommand{\RuleA}[3]{#1 \rightarrow #2^*\ #3}
\newcommand{\RuleB}[3]{#1 \rightarrow #2\ #3^*}
\newcommand{\BinFN}[1]{\mathrm{bin}({#1})}
\newcommand{\TagFN}[1]{\mathrm{tag}({#1})}
\newcommand{\WordFN}[1]{x_{#1}}
\newcommand{\Reals}{\mathbb{R}}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Parsing Dependencies}

\author{}
% \author{Author 1\\
% 	    XYZ Company\\
% 	    111 Anywhere Street\\
% 	    Mytown, NY 10000, USA\\
% 	    {\tt author1@xyz.org}
% 	  \And
% 	Author 2\\
%   	ABC University\\
%   	900 Main Street\\
%   	Ourcity, PQ, Canada A1A 1T2\\
%   {\tt author2@abc.ca}}


%% For drawing the traps/tris.

\newcommand{\abox}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (0,1.5cm);
    \coordinate (C) at (-2cm, 1.5cm);
    \coordinate (D) at (-2cm, 0cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\rtrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (2.5,0.9cm);
    \coordinate (D) at (0:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}


\newcommand{\ltrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (-2.5,0.9cm);
    \coordinate (D) at (180:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\tri}{\Span{\Left{m}, \Right{m}}}

% \newcommand{\tri}{\scalebox{0.2}{\tikz{
%     \coordinate (A) at (0,0);
%     \coordinate (B) at (45:1.5cm);
%     \coordinate (C) at (0:2.25cm);
%     \draw[line width = 0.05cm] (A)--(C)--(B)--cycle;
%     }}}


\newcommand{\rtrapskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.3 with {\arrow[scale=2.75]{<}},
          mark=at position 0.55 with {\arrow[scale=2.75]{<}},
          mark=at position 0.8 with {\arrow[scale=2.75]{<}}
        }]
        \coordinate (A) at (0,0);
        \coordinate (B) at (90:1.5cm);
        \coordinate (C) at (2.5,0.9cm);
        \coordinate (D) at (0:2.5cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
      \end{scope}
    }}}

\newcommand{\rtriskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.7 with {\arrow[scale=2.75]{|}}
        }]
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
      \end{scope}
    }}}

\newcommand{\rtri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\newcommand{\ltri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\date{}

\begin{document}
\maketitle
\begin{abstract}

  Motivated by the increase in accuracy of statistical dependency
  parsers, we consider the problem of decoding phrase-structure
  parses directly from predicted dependency trees. Unlike past rule-based
  approaches, we treat this as a structured prediction problem and
  train a constrained context-free parser for this task. Since our
  parser is constrained by the dependency structure it is both asymptotically
  and empirically faster than standard lexicalized parsers. However,
  despite its simplicity, it still yields high-accuracy phrase-structure parses
  on experiments in both English and Chinese.



\end{abstract}



\section{Introduction}


% There has been a surge of recent work on dep parsing

There are two dominant grammatical
frameworks used for statistical syntactic parsing: phrase-structure and
dependency parsing \cite{}. The two formalisms offer a
trade-off. Phrase-structure parsing is very accurate and provides a full
context-free grammatical representation; while dependency parsing is
much faster, both asymptotically and empirically, while still
predicting much of the important structural relationships in a
sentence.

Recent advances in dependency parsing have lead to
models that perform nearly as well as phrase-structure parsers in
terms of dependency accuracy  \cite{}, and \cite. Table~\ref{fig:depcomp}
shows that for Collins head rules \cite{}, the two state-of-the-art dependency parsers score only \% below the best phrase-structure parsers.

% State-of-art dependency parsers
% such as TurboParser \cite{}, and \cite
% terms of accuracy than large-scale reranking dependency parsers, while
% still maintaining efficient run-time.
-

% Since statistical phrase-structure parsers often use an internal
% lexicalized representation, it is possible to directly compare the
% performance of the two models in terms of dependency accuracy.
% \cite{kong2014empirical} run this comparison across a wide-range of
% popular freely available parsing models and find that while this
% trade-off is still true, the gap in accuracy has significantly
% narrowed over the last several years.

\begin{figure}
  \centering

  \vspace{-1cm}

  \begin{dependency}[theme=simple]
    \begin{deptext}[column sep=0.7cm]
      I$_1$ \& saw$_2$ \& the$_3$ \& man$_4$ \\
    \end{deptext}
    \deproot{2}{ROOT}
      \depedge{2}{1}{}
      \depedge{4}{3}{}
      \depedge{2}{4}{}
  \end{dependency}

  \scalebox{0.7}{
    \Tree [ .X(2) [ .X(1) [ .N I$_1$ ] ]  [ .V saw$_2$ ] [ .X(4) [ .D the$_3$ ]  [ .N man$_4$ ] ] ]
    \Tree [ .X(2)  [ .N I$_1$ ] [ .X(2)  [ .V saw$_2$ ] [ .X(4) [ .D the$_3$ ]  [ .N man$_4$ ] ] ] ]
    \Tree [ .X(2) [ .X(2) [ .N I$_1$ ]   [ .V saw$_2$ ] ]  [ .X(4) [ .D the$_3$ ]  [ .N man$_4$ ] ] ]
  }
  \label{fig:inverse}
  \caption{{\footnotesize While a phrase-structure parse determines a unique dependency parse, the inverse problem is non-deterministic. The figure, adapted from \cite{collins1999statistical}, shows several X-bar trees that all produce the same dependency structure. The parentheses $X(h)$ indicate the head $h$ of each internal vertex. }}
\end{figure}

However, dependencies alone provide less value than a full
phrase-structure parse, and many applications still rely on having the
phrase-structure \cite{}. But while the transformation from
phrase-structure to dependencies is deterministic, the inverse is not, as illustrated in Figure~\ref{fig:inverse}.

We explore the problem of recovering the
phrase-structure from the dependency representation.  This recovery is
challenging for several reasons: (a) there are an exponential number
of phrase-structure trees for any dependency parse, (b) in addition to
the tree structure, it is necessary to recover the non-terminal
symbols of the tree, and (c) the parser must be robust to errors in
the downstream predicted dependency tree.


% Because of these issues is not obvious how to directly perform this conversion without doing search over possible structures.



% \todo[inline]{name?}

% \textsc{Miniparser}

We treat this task as a  structured prediction problem,
and train a complete phrase-structure parser to predict the
syntactic tree for a given sentence. However, we limit the
search space of the parser to the skeleton of a given dependency
tree. Experiments show that


% To address these issue, we build a complete phrase-structure parser
% that is constrained to the skeleton of a given dependency structure.


% To approach this question we present a very simple statistical parser. The parser is a complete exact
% lexicalized parser; except that unlike standard parsers, it takes both
% a sentence and a dependency tree as input. Using this non-standard setup
% has several advantages:

\begin{itemize}
\item The constrained parser is asymptotically  faster than
standard phrase-structure parser for lexicalized context-free grammar.
A standard algorithm for this problem is $O(n^5 |\rules|)$, but with
constrained dependency structure the worst case $is O(n^2 |\rules|)$.

\item In practice using simple pruning that parser is
linear time in the length of the sentence and as efficient as the fastest
high-accuracy dependency parsers. \%

\item Despite being constrained to hard downstream dependency decisions,
the parser is comparably accurate to non-reranked phrase-structure parsers. \%

% \item The framework can be used...

% \cite{}
% \cite{}
% \cite{}

\end{itemize}






% In this paper we ask a simple question: can we invert this process?
% Given the raw skeleton of a dependency parse, is it plausible to obtain
% an accurate phrase-structure parse for a sentence.





% This work has brought the accuracy of dependency parsing much closer to the level
% of powerful phrase- parser. For instance the parser of \cite{}
% score \% on dependency UAS whereas the has UAS etc.


The problem of converting dependency to phrase-structured trees has
been studied previously from a treebanking perspective.
\newcite{xia2001converting} and \newcite{xia2009towards} develop a
rule-based system for the converting human-annotated dependency
parses. In this work we learn the conversion from data, and consider
the case of automatically predicted parses.

This task is similar to phrase-structure parsers that utilize
dependency parsing techniques. \newcite{carreras2008tag} build a
high-accuracy parser that uses a dependency parsing model both for
pruning and within a richer lexicalized parser. Similarly
\newcite{rush2010dual} use dual decomposition to combine a dependency
parser with a simple phrase-structure model. However, we take this
approach a step further by fixing the dependency structure
entirely.


Finally there have also been several papers that use ideas from
dependency parsing to simplify and speed up phrase-structure prediction.
\newcite{zhu2013fast} build a high-accuracy phrase-structure parser
using a transition-based system. \newcite{hall2014less} use a stripped
down parser based on a simple X-bar grammar and a small set of lexicalized features.




%  popular from dependency parsing, and apply it
% constituency parsing.
% Our work is differs in that it look at a more stripped-down problem.
% We give up entirely on determining the structure of the tree itself and only
% look at  producing the nonterminals of the tree.

% \section{Overview}

% In recent years there has been a steady progression in the speed and accuracy of dependency parsers \cite{}; however
% there are still many tasks that rely heavily on the full context-free parses \cite{}. It is therefore desirable
% to produce context-free parses directly from dependency structures.



\begin{table}
  \centering
  \begin{tabular}{|l|lll|}
    \hline
    Model & sec 22 UAS & oracle score & speed \\
    \hline

    \hline
    Berkeley &   $[O(n^5)]$ &  100\% & speed  \\
    stanford  & $[O()]$ & oracle  &  \\
    TurboParse & & & \\
    \hline
  \end{tabular}
  \caption{Collins Head Rules scores on Dev.  }
  \label{fig:depcomp}
\end{table}




% \section{Overview}

% The parser
\section{Background}

We begin by developing notation for a lexicalized phrase-structure formalism and for dependency parsing. The notation aims to highlight the similarity between the two formalisms.

\subsection{Lexicalized CFG Parsing}

A lexicalized context-free grammar (LCFG) is an extended context-free grammar where each vertex in a parse has a unique lexical head word. Define an binarized LCFG as a 4-tuple $(\nonterms, \rules, \terms, \Root)$ where:
\begin{itemize}
\item $\nonterms$; a set of nonterminal symbols, e.g. \Tag{NP}, \Tag{VP}.
\item $\terms$; a set of terminal symbols, often consisting of the words in the language.
\item $\rules$; a set of lexicalized rule productions either of the form $\RuleA{A}{\beta_1}{\beta_2}$ or $\RuleB{A}{\beta_1}{\beta_2}$  consisting of a parent nonterminal $A \in \nonterms$, a sequence of children $\beta_i \in \nonterms$ for $i \in \{1\ldots 2\}$, and a distinguished head child annotated with $*$. The head child comes from the head rules associated with the grammar.
\item $\Root$; a distinguished root symbol $\Root \in \nonterms$.
\end{itemize}

Given an input sentence $x_1, \ldots, x_n$ of terminal symbols from $\terms$, define $\mathcal{Y}(x)$ as the set of valid lexicalized parses for the sentence. This set consists of all binary ordered trees with fringe $x_1, \ldots,  x_n$, internal nodes labeled from $\nonterms$, all tree productions  $A \rightarrow \beta$ consisting of members of $\rules$, and root label $\Root$.


% Given fixed sequence of terminal symbols $x_1 \in \terms, \ldots, x_n \in \terms$, for instance a sentence of words, a context-free parse consists of any ordered, finite-size tree where the interior labels come $\nonterms$, the fringe labels correspond to the initial sequence, and all tree productions

For an LCFG parse $y \in \mathcal{Y}(x)$,
we further associate a triple $v = (\Span{i, j}, h, A)$ with each vertex in the tree, where
% We define the following important properties of this parse tree for each vertex:


\begin{itemize}
\item $\Span{i,j}$; the \textit{span}  of the vertex, i.e. the contiguous sequence $\{x_i, \ldots, x_j\}$ of the sentence covered by the vertex.

\item $h(v) \in \{1, \ldots n\}$; index indicating that $x_h$ is the \textit{head} of the vertex, defined recursively by the following rules:
  \begin{enumerate}
  \item  If the vertex is leaf $x_i$, then $h=i$.
  \item Otherwise,  $h$ matches the head child where $\RuleA{A}{\beta_1}{\beta_2}$ or $\RuleB{A}{\beta_1}{\beta_2}$  is the rule production at this vertex.
  \end{enumerate}

\item $A \in \terms \cup \nonterms$; the terminal or nonterminal symbol of the vertex.
\end{itemize}

Note that each word $x_i$ but one has an ancestor vertex $v$ where $h(v) \neq i$.  Define the
\textit{spine} of word $x_i$ to be the longest chain connected vertices $v_1,
\ldots, v_p$ where $h(v_j) = i$ for $j \in \{1, \ldots, p\}$.
Also if it exists, let vertex $v_{p+1}$  be the parent of vertex~$v_p$,
where $h(v_{p+1}) \neq i$. The full notation is illustrated in Figure~\ref{fig:spine}.

\begin{figure}
  \centering


  \Tree [ .\node[color=red]{$(\Span{1,n}, 3, \Tag{S})$}; [ .\node[color=blue]{$(\Span{1,2}, 2,  \Tag{NP})$}; [  .\node{$(\Span{1,1}, 1,  \Tag{DT})$}; The$_1$ ]  [ .\node[color=blue]{$(\Span{2,2}, 2, \Tag{NN})$}; \textcolor{blue}{automaker$_2$} ] ] [ .\node{$(\Span{3,3}, 3,  \Tag{VBD})$}; sold$_3$ ] [ .\node{..}; ] ]



  \begin{dependency}[theme=simple]
    \begin{deptext}[column sep=0.7cm]
      The$_1$ \& automaker$_2$ \& sold$_3$ \& $\ldots$ \\
    \end{deptext}
    \deproot{3}{}
      \depedge{2}{1}{}
      \depedge{3}{2}{}
  \end{dependency}


  % \Tree [ .A [ .B  C ] ]
  \caption{Figure illustrating an LCFG parse. The parse is an ordered tree with fringe $x_1, \ldots, x_n$. Each vertex is annotated with a span, head, and syntactic tag. The blue vertices represent the 3-vertex spine $v_1, v_2, v_3$ of the word \texttt{automaker$_2$}. The root vertex is $v_4$, which implies that \texttt{automaker$_2$} modifies \texttt{sold$_3$} in the induced dependency graph.     }
  \label{fig:spine}
\end{figure}

\subsection{Dependency Parsing}

Dependency trees provide an alternative, and in some sense simpler,
representation of grammatical structure.


Given an input sentence $x_1
\ldots x_n$, define a dependency parse $d$ as a sequence $d_1 \ldots d_n$ where for all $i$, $d_i \in \{0, \ldots, n\}$. These dependency relations can be seen as arcs $(d_i, i)$ in a directed graph over the word indices where $0$ is a special pseudo-root vertex.  A dependency parse is valid  if the corresponding directed graph is a directed tree rooted at vertex~$0$. Figure~\ref{fig:spine} contains an example of a dependency tree.
% to be the head of the sentence, i.e. it modifies $g_i \in \{0, n\}$
% where $0$ is a special pseudo-root symbol.
% If each word and the pseudo-root is represented as a vertex, than
% these relations correspond to arcs $(g_i, i)$ in a directed graph

For a valid dependency tree, define the \textit{span} of any word $x_m$ as the set of indices reachable from vertex $m$ in the directed tree. A dependency parse is \textit{projective} if the descendants of every word in the tree form a contiguous span of the original sentence \cite{}. We use the notation $\Left{m}$ and $\Right{m}$ to represent the left- and
right-boundaries of this span.

% The highest-scoring projective dependency parse under an arc-factored scoring function can be found in time $O(n^3)$.





% define a sequence of dependencies $()$

% define a sequence $g_1 \ldots
% g_n$ where for all $i$, $t_i$ is the dependent of position $g_i \in  \{0,
% \ldots n\}$, and $0$ is a special pseudo-root symbol.

% We now give the important property for this work.

Any lexicalized context-free parse can be converted to a unique projective dependency tree.
For an input symbol $x_m$ with spine $v_1, \ldots, v_p$,

\begin{enumerate}
\item If $v_p$ is the root of the tree,
then $d_m = 0$.
\item Otherwise let $v_{p+1}$ be the parent vertex of
$v_p$ and $d_m = h(v_{p+1})$. The span $\Span{i, j}$ of $v_p$ in the lexicalized parse is equivalent to $\Span{\Left{m}, \Right{m}}$
in the induced dependency parse.
\end{enumerate}

% In the binarized case,


The conversion produces a directed tree rooted by preserving the tree structure of the original LCFG parse.

However the reverse conversion is not unique, and in fact, it can be shown that in the worst-case there are an
exponential number of possible phrase structure trees that match a given dependency parse. We give the proof in
Appendix~\ref{app:proof}.




% Any lexicalized







% For a specific parse $p$ the head rules induce a projective dependency parse over the sentence.
% To see this, as a base case associate a head word $i$ with each node $(i, i)$ in the tree.
% Then inductively at each inner node with span $(i,j)$ assign the head word $i \leq h \leq$ where
% $h$ is the head word of the $H(r)$th child node.

% The span of a word $i$ is the span $(i, j)$ of $p_n$.


% Define the spine of a word $i$ as the sequence of nodes $p_1, \ldots p_n$ such that $h(p_j) = i$.
% Let $p_{n+1}$ be the parent of $p_n$, then we say that $i$ is the dependent of $h(p_{n+1})$. By convention
% if $p_n$ is the root of the tree then $i$ is the dependent of the pseudo-root $0$.

% We can think of these dependencies as a directed graph over the nodes $\{0, \ldots n\}$ where the arc $(j, i)$ indicates that
% word $i$ is a dependent of word $j$.


% $l,r:\{1\ldots n\} \mapsto \{1\ldots n\}$


% While we have described dependencies as the deterministic by-product
% of phrase-structure parsing, in recent years there has been extensive
% work on directly predicting the dependency structure $(j,i)$ without
% predicting the full parse structure. The main benefit of this approach
% is that these structures can be predicted efficiently which has allowed
% for extensive use of powerful structured prediction techniques.






% A labeled dependency parse is one with a set $\mathcal{L}$
% and $(i, j, l)$.


% \subsection{Dependency Parsing}


% A dependency parse is any directed tree over this graph rooted
% at node $0$.


% If there exists a path from $w_i$ to $w_j$ than $w_j$ is a
% an ancestor of word $i$.

% Define a \textit{projective} dependency parse as one where for any word
% the ancestors are contiguous.

% We can specify a dependency parse as a sequence $(i,j)$.


\section{Parsing Dependencies}

Our main problem of interest is to predict the best
phrase-structure tree for a given dependency parse.
We frame this problem as a combinatorial decoding problem.
First, we describe the standard lexicalized CKY algorithm
and then give a constrained version of this algorithm.


 % To solve this inverse problem we set up as simple variant of the
% original lexicalized parsing algorithm. While this algorithm has the
% same form of the original, we show that it can be solved
% asymptotically faster.




\subsection{Constrained Parsing Algorithm}

% We will assume in this section that the LCFG is in lexicalized Chomsky normal form,
% with all non-preterminal rules of the form $\Rule{A}{\beta_1}{\beta_2}$.
% In the next section we show how to convert our original grammar to this form while
% preserving the head structure.

For a given lexicalized context-free grammar,
define the set of valid parses for a sentence as $\mathcal{Y}(x)$.
The parsing problem is to find the highest-scoring parse in this set, i.e.  \[ \hat{y} \gets \argmax_{y \in {\cal Y}(x)} s(y;x) \] where
$s$ is a scoring function.

If the scoring function factors over rule productions, then
the highest-scoring parse can be found using the lexicalized
CKY algorithm. This algorithm is defined as a
collection of inductive rules shown in Figure~\ref{fig:cky}.

For example consider the inductive rule

\[ \infer{(\Span{i, j}, h, A)}{(\Span{i, k}, m, \beta_1) &  (\Span{k+1, j}, h, \beta_2)} \]
 for all rules $\RuleA{A}{\beta_1}{\beta_2}\in \rules$ and spans $i \leq k < j$. This rule indicates that grammatical rule $\RuleA{A}{\beta_1}{\beta_2}$  was applied at a vertex covering $\Span{i, j}$ to produce two vertices covering $\Span{i, k}$ and $\Span{k+1, j}$.

 The best parse can found by bottom-up dynamic programming
 over this set. The running time of this algorithm is linear in the
 number of rules. The standard algorithm requires $O(n^5 |\rules|)$ time, which is intractable to run without heavy pruning.



 However, in this work, we are interested in a constrained variant of
 this problem. We assume that we additionally have access to a
 projective dependency parse for the sentence, $d_1, \ldots, d_n$.
 Define the set $\mathcal{Y}(x,d)$ as all valid LCFG parses that match
 this dependency parse. For all inductive rules with head $h$ and modifier
$m$, there must be a dependency $(h, m)$ in $d$.  Our aim is to find

\[ \argmax_{y \in \mathcal{Y}(x, d)} s(y; x, d)\]

This new problem has a nice property. For any word $x_m$ with spine $v_1 \ldots v_p$ the LCFG span  $\Span{i,j}$ of $v_p$ is equal to the dependency span $\langle \Left{m},\Right{m}\rangle$ of $x_m$. Furthermore these spans can be easily computed directly from given $d$.

This property greatly limits the search space of the parsing problem.
Instead of searching over all possible spans $\Span{i, j}$ of each modifier, we can
precompute $\langle \Left{m},\Right{m}\rangle$. Figure~\ref{fig:cky_new} shows the new set inductive rules.

While these rules are very similar to the original, the quantifier are
much more constrained. Given that there are $n$ dependency links $(h, m)$ and $n$ indices, the new algorithm has $O(n^2|\rules|)$ running time.



% Now we make a simple extension to this algorithm. In the last section $x$ was just the sentence, now we assume that we also are given the dependency parse. Define ${\cal Y}(x)$ to the be the set of parse tree that agree with the dependency structure.


% That is if a rule in the parse $y \in \mathcal{Y}$ then the implied dependency is $(h, m)$ is in the dependency parse. The new problem is to solve

% \[ \argmax_{y \in {\cal Y}(x)} s(y;x) \]

%
% Recall that $\Left{m}$ and $\Right{m}$ are the precomputed left- and right- boundary words of the cover of word $m$.

% While the algorithm is virtually identical to the standard lexicalized CKY algorithm, we are now able to fix many of the free indices.

\begin{figure*}
  \begin{multicols}{2}
  \noindent \textbf{Premise:}
  \[(\Span{i, i}, i, A)\ \ \ \forall i \in \{1 \ldots n\}, A \in {\cal N}\]

  \noindent\textbf{Rules:}


   For $i\leq h \leq k < m \leq j$,  and  rule  $\RuleA{A}{\beta_1}{\beta_2}$,
   \[\infer{( \Span{i, j},  h,  A)}{( \Span{i, k}, h, \beta_1)  &  ( \Span{k+1, j}, m, \beta_2) } \]

   For $i\leq m \leq k < h \leq j$, rule  $\RuleB{A}{\beta_1}{\beta_2}$,
   \[\infer{(\Span{i, j},  h, A)}{(\Span{i, k}, m, \beta_1)  &  (\Span{k+1, j}, h, \beta_2) }  \]

\noindent \textbf{Goal:}\[ (\Span{1, n}, m, \Root) \mathrm{\ for\ any }\ m\]
\label{fig:cky}

% \end{figure}

% \begin{figure}
  \noindent \textbf{Premise:}
  \[(\langle i,i \rangle, i, A)\ \ \ \forall i \in \{1 \ldots n\}, A \in {\cal N}\]

  \noindent\textbf{Rules:}

   % For all    $h$,  nonterminals $A \in \nonterms$,
   % \[ \infer{( \tri, h, A)}{(\Span{\Left{h}, \Right{h}}, h, A) } \]


   For all   $i < m, h = d_m$  and rule  $\RuleA{A}{\beta_1}{\beta_2}$,
  \[\infer{( \Span{ i, \Right{m} }, h, A)}{(\Span{i, \Left{m}-1}, h, \beta_1)  &  (\tri, m, \beta_2) } \]

  For all    $m < j, h = d_m$ and  rule  $\RuleB{A}{\beta_1}{\beta_2}$,
  \[ \infer{( \Span{\Left{m}, j}, h, A)}{(\tri, m, \beta_1)  &  (\Span{\Right{m}+ 1, j}, h, \beta_2) } \]


% \forall  i, j, h, A \rightarrow B^* \\\\
%   \infer{(\Span{i, j}, h, A)}{(\Span{i, j}, h, \beta_1)}  \\\
% \\

\noindent \textbf{Goal:}\[ (\Span{1, n}, m, \Root) \mathrm{\ for\ any }\ m \mathrm{\ s.t. \ } d_m = 0  \]
\end{multicols}
\label{fig:cky_new}
\caption{(a) Standard CKY algorithm for LCFG parsing stated as inductive rules. Starting from the \textit{premise}, any valid application of \textit{rules} that leads to a \textit{goal} is a valid parse. Finding the optimal parse with dynamic programming is linear in the number of rules. For this algorithm there are $O(n^5|\rules|)$ rules where $n$ is the length of the sentence. (b) The constrained CKY parsing algorithm for $\mathcal{Y}(x, d)$. The algorithm is nearly identical to Figure~\ref{fig:cky} except that many of the free indices are now fixed to the dependency parse. Finding the optimal parse is now $O(n^2|\rules|)$}.
\end{figure*}

% Here we assume there can




\subsection{Extension: Labels}

\todo[inline]{Finish this section}

Standard dependency parsers also predict labels from a set ${\cal L}$ on each dependency link.
In a labeled dependency parser a would be of the form $(i, j, l)$.

This label information can be used to encode further information about the parse structure. For instance
if we use the label set ${\cal L} = \nonterms \times \nonterms \times \nonterms$, encoding the binary rule decisions $\Rule{A}{\beta_1}{\beta_2}$.


% \subsection{Extension: Pruning}

\subsection{Extension: Pruning}
\label{sec:prune}

We also experiment with a simple pruning dictionary pruning technique.
For each context-free rule $A \rightarrow \beta_1\ \beta_2$ and POS tag $a$ we
remove all rules that were not seen with that tag as a head in training.


Pruning discussion


\begin{table}
  \centering
  \begin{tabular}{|l|lll|}
    \hline
    Model & \% items pruned & oracle score & speed \\
    \hline

    \hline
    LCFG &   $[O(n^5)]$ &  100\% & speed  \\
    Dep Parse Model  & $[O()]$ & oracle  &  \\
    Pruned Model & $O(n)$ &  pruned oracle & \\
    \hline
  \end{tabular}
  \caption{Pruning }
\end{table}


\subsection{Binarization}

In order to have an efficient binary LCFG grammar, we must convert the
non-binary treebank grammars to binary form.  While the algorithm
itself is not dependent on the binarization used, this choice affects
the run-time of the algorithm, through $\rules$, as well as the
structure of the scoring function.

Our binarization decomposes non-binary rules into fragments for
each head-modifier pair.

% A general LCFG can be binarized while still maintaining the lexical
% structure and making it possible to recover the original tree.
% Since the parser traces a fixed dependency structure
% we select a binarization based around the head
% structure.

For simplicity, we consider binarizing rule $\langle A \rightarrow \beta_1 \ldots \beta_m,
k\rangle$ with $m > 2$. Relative to the head $\beta_k$
the rule has left-side $\beta_1 \ldots \beta_{k-1}$ and right-side
$\beta_{k+1} \ldots \beta_m$.

We replace this rule with binary rules that consume each side
independently as a first-order Markov chain (horizontal Markovization).
The main transformation is to introduce rules

\begin{itemize}

\item
$\Rule{A^{\Rightarrow}_{\beta_i}}{A^{\Rightarrow*}_{\beta_{i-1}}}{\beta_i}$ for $k < i < m$

\item
$\Rule{A^{\Leftarrow}_{\beta_i}} {\beta_i}{A^{\Leftarrow*}_{\beta_{i+1}}} $ for $1< i < k$
\end{itemize}


% As an example consider the rule

Additionally we introduce several additional rules to handle the boundary cases of starting a new rule, finishing the right side, and completing a rule. (These rules are slightly modified when $k\leq 2$ or $k=m$).
\begin{align*}
& \RuleA{A^{\Rightarrow}_{\beta_{k+1}}}{\beta_k}{\beta_{k+1}} & \Rule{A^{\Rightarrow*}} {A^{\Rightarrow}_{\beta_{m-1}}}{\beta_m} \\
 &\RuleB{ A^{\Leftarrow}_{\beta_{k-1}}}{\beta_{k-1}}{A^{\Rightarrow}} & \RuleB{A}{\beta_1}{ A^{\Leftarrow}_{\beta_{2}}}
\end{align*}

\noindent For example the transformation of a common rule looks like

\begin{center}
  \scalebox{0.6}{ \Tree [ .S NP VP$^*$ NP NP ] $\Rightarrow$ \Tree [
    .S NP [ .S$^{\Rightarrow*}$ [ .S$^{\Rightarrow*}_{NP}$ VP$^*$ NP ]
    NP ] ] }
\end{center}

\noindent Each rule contains at most 3 original nonterminals so the size of the new binarized rule set is bounded by $O(\nonterms^3)$.

% \begin{figure}
%   \centering
%   \scalebox{0.6}{
%   \Tree [ .S NP VP$^*$ NP NP ]
%   $\Rightarrow$
%   \Tree [ .S NP   [ .S$^{\Rightarrow*}$ [ .S$^{\Rightarrow*}_{NP}$ VP$^*$  NP ]  NP ]   ]
% }
%   \label{fig:rule}
% \end{figure}




% \subsection{Complexity Analysis}

% Since we know the dep tree, there are exactly $n$ $h \rightarrow m$ arcs. Therefore there are $O(n^2 |{\cal R}|)$ binary terms.


% The unary terms are more difficult to limit. Worst case it will have $O(n^3 |{\cal R}_1|)$ terms where ${\cal R}_1$ is the set of unary rules.
% In practice there will be $O(n \times \max_h({\#l_h \times  \#r_h}) \times |{\cal R}_1|)$ terms where $\#r_h$ and $\#l_h$ are the number of left and right modifiers
% of $h$ respectively.

\section{Structured Prediction}

To learn the scoring function for the transformation from dependency trees to phrase-structure trees,
we use a standard structured prediction setup.
We define the scoring function $s$ as

\[s(y;x, d, \theta) =  \theta^{\top} f(x, d, y) \]

\noindent
where $\theta \in \Reals^D $ is a weight vector  and $f(x, d, y)$ is a feature function that maps parse production (as in Figure~\ref{}) to sparse feature vectors in $\{0,1\}^D$. In this section we first discuss the features used and then training for the weight vector.


\subsection{Features}

We implemented a small set of standard dependency and phrase-structure features.

For the dependency style features, we replicated the basic arc-factored features
used by \newcite{mcdonald2006discriminative}. These include combinations of:

\begin{itemize}
\item nonterminal combinations
\item rule and top nonterminal
\item modifier word and part-of-speech
\item head word word and part-of-speech
\end{itemize}

Additionally we included the span features described for the X-Bar style
parser of \newcite{hall2014less}. These include conjunction of the rule
with:

\begin{itemize}
\item first and last word of current span.
\item preceding and following word of current span
\item adjacent words at split of current span
\item length of the span
\end{itemize}


The full feature set  is shown in Figure~\ref{fig:features}.
After training there are \# non-zero features.


% We experimented with several settings of the part features.
% The final features are shown in  Figure~\ref{fig:features}.

% The first set of features look at the rule structure $\Rule{A}{\beta_1}{\beta_2}$.
% We have features on several combinations of rule and non-terminals.

% The next set of features look at properties of the lexicalization.
% These look at combinations of the head word and tags.






% \cite{}
% \cite{}
% \cite{}



\begin{figure}
  \footnotesize
  \centering
  For a part $ \infer{(\Span{i, j}, h, A)}{(\Span{i, k}, m, \beta_1) &  (\Span{k+1, j}, h, \beta_2)} $

  \vspace{0.5cm}

  \begin{multicols}{2}

  \begin{tabular}{|l|l}

  \hline
  Nonterm Features \\
  \hline

  \hline
  $(A$, $\beta_1)$ \\
  $(A$, $\beta_2)$ \\
  $(A, \beta_1, \TagFN{m})$ \\
  $(A, \beta_2, \TagFN{h})$ \\
  \hline
    \hline
  Span Features \\
  \hline

  \hline
  $(\RuleSym, \WordFN{i})$\\
  $(\RuleSym, \WordFN{j})$\\
  $(\RuleSym, \WordFN{i-1})$\\
  $(\RuleSym, \WordFN{j+1})$\\
  $(\RuleSym, \WordFN{k})$\\
  $(\RuleSym, \WordFN{k+1})$\\
  $(\RuleSym, \BinFN{j-i})$\\
  \hline

  \end{tabular}

  \begin{tabular}{|l|l}

  \hline


  Rule Features \\
  \hline

  \hline

  $(\RuleSym  )$\\
  $(\RuleSym, \WordFN{h}, \TagFN{m})$ \\
  $(\RuleSym, \TagFN{h}, \WordFN{m})$ \\
  $(\RuleSym, \TagFN{h}, \TagFN{m})$ \\

  $(\RuleSym, \WordFN{h})$ \\
  $(\RuleSym, \TagFN{h})$ \\
  $(\RuleSym, \WordFN{m})$ \\
  $(\RuleSym, \TagFN{m})$ \\

  \hline

  \end{tabular}
  \end{multicols}
  \label{fig:features}
  \caption{The feature templates used in the function $f(x, d, y)$ . The symbol $\RuleSym$ is expanded into two conjunction $\Rule{A}{B}{C}$ and $A$. The function $\TagFN{i}$ gives the part-of-speech tag of word $x_i$. The function $\BinFN{i}$ bins a span length into 10 bins.
  }
\end{figure}

\subsection{Training}

We train the parameters $\theta$ using standard structured SVM training \cite{}.

We assume that we are given a set of gold-annotated parse examples: $( x^{1}, y^{1}), \ldots,  (x^{D}, y^{D})$. We also define $d^{(1)} \ldots d^{(D)}$ as the dependency structures induced from $y^{1} \ldots y^{D}$.
We select parameters to minimize the regularized empirical risk

\[ \min_{\theta} \sum_{i = 1}^D \max\{0,  \ell( x^{i}, d^{i} , y^{i}, \theta) \} + \frac{\lambda}{2} ||\theta||_1 \]

\noindent where we define $\ell$ as

\[\ell(x, d, y, \theta) = s(y) + \max_{y' \in \mathcal{Y}(x, d)}\left(s(y')  + \Delta(y, y') \right) \]


\noindent where $\Delta$ is a problem specific cost-function that we assume is linear in either arguments.
In experiments, we use a hamming loss  $\Delta(y, \bar{y}) = || y - \bar{y}||$ where $y$ is an indicator
of rule productions.

The objective is optimized using Adagrad \cite{}.  The gradient
calculation requires computing a loss-augmented argmax for each
training example which is done using the algorithm of Figure~\ref{}.

% We consider two extensions to the standard setup.

% First is that at training time we do not have access to the true dependency structure $d$, but instead only predicted dependencies $\hat{d}$. Past work on parsing  \cite{} has shown that it can be important to train using these predicted structures.

% However, using $\hat{d}$ may make it impossible to recover the gold LCFG parse $y$, e.g. $y \not \in \mathcal{Y}(x, \hat{d})$. To handle this issue, we also replace $y$ with the oracle parse $\bar{y}$ defined at the projection of $y$ onto $\mathcal{Y}(x, \hat{d})$

% \[ \bar{y} \gets \argmin_{y' \in \mathcal{Y}(x, \hat{d})} \Delta(y', y) \]

% If $\Delta$ is linear in its first argument, this projection can be found by running the same algorithm as in Figure~\ref{}.

% We replace the loss with $\ell(x^{(i)}, \hat{d}^{(i)}, \bar{y}^{(i)})

% % \[l(i, \theta) = s(\bar{y}^{(i)}) + \max_{y \in \mathcal{Y}(x^{(i)}, \hat{d}^{(i)})}\left( s(y)  + \Delta(\bar{y}^{(i)}, y) \right) \]






% One issue with directly minimizing this objective, is that

%  for this problem is the role that
% parsing issues play. If there is a dependency parsing mistake makes it often impossible
% for the system to recover the correct parse.


% Define a loss function $ \Delta(\hat{y}, y)$ to determine the difference between two parses.
% We assume that  the loss function is linear in its first argument.




% Define the oracle parse to be the parse minimizes the loss function


% Since this functions is linear we can use our

% To learn the weight vector $\theta$ we optimize a standard structured svm objective




% Note that we are optimizing towards the oracle parse.


\section{Setup}


\subsection{Data and Methods}

For English experiments we use the standard Penn Treebank (PTB)
experimental setup \cite{marcus1993building}. Training is done on
section 2-21, development on section 22, and test of section 23.

For Chinese experiment, we the standard Chinese Treebank 5.1 (CTB)
experimental setup \cite{xue2005penn}.

Part-of-speech tagging is done using TurboTagger
\cite{martins2013turning}. Prior to training, the train sections are
automatically tagged using 10-fold jack-knifing. At training time, the
gold dependency structures are computed using the Collins head rules
\cite{collins2003head}.\footnote{ We experimented with using
  jackknifed dependency parses $d'$ at training time with oracle tree
  structures, i.e. $\argmin_{y' \in \mathcal{Y}(x, d')} \Delta(y, y)$,
  but found that this did not improve performance.}

Evaluation for phrase-structure parses is performed using the \texttt{evalb}\footnote{http://nlp.cs.nyu.edu/evalb/} script using a standard setup and for dependency parsing using unlabeled accuracy score (UAS).

We implemented the grammar binarization, head rules, and pruning
tables in Python, and the parser, features, and training in C++. The
core run-time decoding algorithm is self contained and requires less
than 500 lines of code. Both are publicly available.\footnote{Withheld
  for review}

% Parser is in C++, publicly available, 500 lines of code..
% At training time, we run 10-fold jack-knifing to produce dependency parses $\hat{d}$. We then run a single pass to calculate $\bar{y}$ for each training example.






% \subsection{Binarization}

% Before describing our parsing algorithm we first describe a binarization approach to make efficient parsing possible and highlight the relationship between the LCFG and the dependency parse.

% We can binarize a context-free grammar to produce a new grammar where for all rules $A \rightarrow \beta$ the length of $\beta$ is in either $\{1, 2\}$
% in such a way that we can obtain the original tree.

% We first describe our binarization approach. Our aim is for every non-unary rule to correspond to exactly one dependency arc.

% \subsection{}

\section{Experiments}

We ran experiments to assess the accuracy of the method, its run-time efficiency, the amount of phrase-structure data required, and the effect of dependency accuracy.

\subsection{Parsing Accuracy}

\begin{table}
  \centering

  \begin{tabular}{|l|lll|}
    \hline
    & \multicolumn{3}{|c|}{PTB} \\
    \hline

    \hline
    model &  dev fscore & & test fscore  \\
    \hline
    Petrov[06] \nocite{petrov2006learning} & & & \\
    Carreras[08] \nocite{carreras2008tag} & & & \\
    Zhu[13] \nocite{zhu2013fast} & &  & \\
    Charniak[00] \nocite{charniak2000maximum} & & & \\
    Stanford[] & & & \\
    \textsc{ParseDep} & & & \\
    \hline
    \hline
    & \multicolumn{3}{|c|}{CTB} \\
    model &  dev fscore & & test fscore  \\
    \hline

    \hline
    Petrov[06] \nocite{petrov2006learning} & & & \\
    Carreras[08] \nocite{carreras2008tag} & & & \\
    Zhu[13] \nocite{zhu2013fast} & &  & \\
    Stanford[] & & & \\
    Charniak[00] \nocite{charniak2000maximum} & & & \\
    \textsc{ParseDep} & & & \\
    \hline
  \end{tabular}
  \label{tab:acc}
  \caption{ Accuracy results on the Penn Treebank and Chinese Treebank datasets. Comparisons are to state-of-the-art non-reranking phrase-structure parsers including:  Petrov[06] \cite{petrov2006learning}, Carraras[08] \cite{carreras2008tag}, Zhu[13] \cite{zhu2013fast}, Charniak[00] \cite{charniak2000maximum}, and Stanford[] \cite{}.    }
\end{table}

Our first experiments, shown in Table~\ref{tab:acc}, examine the accuracy of the phrase-structure trees produced by the parser. For these experiments,
we use TurboParser \cite{martins2013turning} to predict downstream dependencies.

$\ldots$


\subsection{Efficiency}

Our next set of experiments consider the efficiency of the model. For these experiments we consider both the full and pruned version of the parser using the pruning described in section~\ref{sec:prune}. Table~\ref{tab:speed} shows that in practice the parser is quite fast,  averaging around \% tokens per second at high accuracy.

We also consider the end-to-end speed of the parser when combined with different downstream dependencies. We look at

Finally we consider the practical run-time of the parser on sentences of different length. Figure~\ref{} shows the graph.


\subsection{Analysis}


To gauge the upper bound of the accuracy of this system we consider an oracle version of the parser. For a gold parse $y$ and predicted dependencies $\hat{d}$,  define the oracle parse $y'$ as
\[ y' = \argmin_{y' \in \mathcal{Y}(x, \hat{d})} \Delta(y, y')} \]
\noindent Table~\ref{tab:oracle} shows the oracle accuracy of TurboParser and several other commonly used dependency parsers.

We also consider the mistakes that are made by the parser compared to the
mistakes made. For each of the bracketing errors made by the parser, we can classify it as a bracketing mistake, a dependency mistake or neither.


\begin{table}
  \centering

  \begin{tabular}{|l|lll|}
    \hline
    Model & oracle & fscore & speed  \\
    \hline

    \hline
    \textsc{TurboParser}& & & \\
    \textsc{MaltParser}& & & \\
    \textsc{MIT}& & & \\
    \hline
  \end{tabular}

  \vspace{0.5cm}

  \begin{tabular}{|lll|}
    \hline
     oracle & dep & dev  \\
    \hline

    \hline
    & & \\
    \hline
  \end{tabular}
  \label{tab:oracle}
  \caption{(a) Oracle accuracy for the predicted dependency trees of commonly used dependency parsers including TurboParser \cite{martins2013turning}, MaltParser \cite{nivre2006maltparser}, and MIT \cite{}. (b) Classification of the bracketing mistakes made by the parser. }
\end{table}


\subsection{Conversion}

Previous work on this problem has looked at converting dependency trees to phrase-structure trees using linguistic rules \cite{xia2001converting,xia2009towards}. This work is targeted towards the development of treebanks, particularly converting dependency treebanks to phrase-structure treebanks.
For this application, it is useful to convert gold trees as opposed to predicted trees.

To compare to this work, we train our parser with gold tags and run on gold dependency trees in development. Table~\ref{tab:convert} give the results for this task.


\begin{table}
  \centering
  \begin{tabular}{|l|ll|}

    \hline
    model & dev fscore & \\
    \hline

    \hline
    Xia[0] & & \\
    \textsc{ParseDep}  & & \\
    \hline

  \end{tabular}
  \caption{}
  \label{tab:convert}
\end{table}

\subsection{Training Size}

Finally we consider a variant setup. It is often cheaper to develop
dependency treebanks than full phrase-structure treebanks, and they
exists for more languages \cite{}. For these languages we may have a small
amount of phrase-structure annotations and a large amount of dependency annotations.

\begin{table}
  \centering
  \begin{tabular}{|l|ll|}

    \hline
    model & dev fscore & \\
    \hline

    \hline
    \textsc{ParseDep[5\%]}  & & \\
    \textsc{ParseDep[10\%]}  & & \\
    \textsc{ParseDep[20\%]}  & & \\
    \hline

  \end{tabular}
  \caption{}
  \label{tab:size}
\end{table}


To simulate this setup, we consider training a dependency parser on the full PTB, but limit our model to a small slice of the treebank. Table~\ref{size} shows the development set results with different splits of the training data.

$\ldots$

\begin{table}
  \centering
  \begin{tabular}{|l|ll|}
    \hline
    Model & fscore & speed  \\
    \hline
    \textsc{TurboParser} & & \\
    \textsc{MaltParser} & & \\
    \textsc{MIT} & & \\
    \hline
  \end{tabular}

  \missingfigure{Speed length graph}
  \label{tab:speed}
  \caption{Experiments of parsing speed. (a) The speed of the parser on its own and with pruning. (b) The end-to-end speed of the parser when combined with different dependency parsers. }
\end{table}




\section{Conclusion}



\appendix{}

\section{Proof of PS Size}
\label{app:proof}

Consider the LCFG grammar with two rules $A = \RuleA{X}{X}{X}$ and  $ B = \RuleB{X}{X}{X}$ and a sentence $x_1, \ldots, x_{2n+1}$. Let the dependency parse be defined as $d_{n+1} = 0$ and $d_i = n+1$ for all $i \neq n + 1$, i.e.

\begin{center}

\scalebox{0.5}{
\begin{dependency}[theme=simple]
  \begin{deptext}[column sep=0.7cm]
    $1$ \& $2$ \& \ldots \& $n+1$ \& $\ldots$ \& $2n$ \& $2n+1$ \\
  \end{deptext}
  \deproot{4}{ROOT}
  \depedge{4}{1}{}
  \depedge{4}{2}{}
  \depedge{4}{6}{}
  \depedge{4}{7}{}
\end{dependency}
}
\end{center}

\noindent Since all rules have $h = x_n$ as head, a parse is a chain of $2n$ rules with each rule in $\{A, B\}$, e.g. the following are $BB...$, $BA...$, $AA...$

\begin{center}

\scalebox{0.6} {
\Tree [ .X $x_1$ [ .X $x_2$  [ .$\vdots$ $x_{n+1}$ ]   ] ]
\Tree [ .X $x_1$ [ .X  [ .$\vdots$ $x_{n+1}$ ] $x_{2n+1}$  ] ]
\Tree [ .X  [ .X  [ .$\vdots$ $x_{n+1}$ ]   $x_{2n}$ ] $x_{2n+1}$ ]
}
\end{center}


\noindent Since there must be equal $A$s and $B$s and all orders are possible, there are $2n \choose n$ valid parses and $|\mathcal{Y}(x, d)|$ is $O(2^n)$.
% \textbf{Acknowledgment} sections should go as a last (unnumbered) section immediately
% before the references.



\bibliography{full}
\bibliographystyle{naaclhlt2015}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End
