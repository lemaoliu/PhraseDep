%
% File naaclhlt2015.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}

\usepackage{tikz-qtree}
\usepackage{tikz-dependency}
\usepackage{proof}
\usepackage{fullpage}
\usepackage{todonotes}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}


\usepackage{amsfonts}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{algpseudocode}
\usepackage{algorithm}

\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
\tikzset{
  >=latex,text height=1.5ex,text depth=0.25ex
}

\newcommand{\nonterms}{\mathcal{N}}
\newcommand{\END}{\mathrm{END}}
\newcommand{\START}{\mathrm{START}}
\newcommand{\rules}{\mathcal{R}}
\newcommand{\terms}{\mathcal{T}}
\newcommand{\Left}[1]{#1_{\Leftarrow}}
\newcommand{\Right}[1]{#1_{\Rightarrow}}
\newcommand{\Span}[1]{\langle #1 \rangle}
% \newcommand{\root}{r}

\newcommand{\Tag}[1]{\texttt{#1}}
\newcommand{\Root}{r}

\newcommand{\Rule}[3]{#1 \rightarrow #2\ #3}
\newcommand{\RuleA}[3]{#1 \rightarrow #2^*\ #3}
\newcommand{\RuleB}[3]{#1 \rightarrow #2\ #3^*}
\newcommand{\TagFN}[1]{\mathrm{tag}({#1})}
\newcommand{\WordFN}[1]{\mathrm{word}({#1})}
\newcommand{\Reals}{\mathbb{R}}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Parsing Dependencies}

\author{}
% \author{Author 1\\
% 	    XYZ Company\\
% 	    111 Anywhere Street\\
% 	    Mytown, NY 10000, USA\\
% 	    {\tt author1@xyz.org}
% 	  \And
% 	Author 2\\
%   	ABC University\\
%   	900 Main Street\\
%   	Ourcity, PQ, Canada A1A 1T2\\
%   {\tt author2@abc.ca}}


%% For drawing the traps/tris.

\newcommand{\abox}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (0,1.5cm);
    \coordinate (C) at (-2cm, 1.5cm);
    \coordinate (D) at (-2cm, 0cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\rtrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (2.5,0.9cm);
    \coordinate (D) at (0:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}


\newcommand{\ltrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (-2.5,0.9cm);
    \coordinate (D) at (180:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\tri}{\Span{\Left{m}, \Right{m}}}

% \newcommand{\tri}{\scalebox{0.2}{\tikz{
%     \coordinate (A) at (0,0);
%     \coordinate (B) at (45:1.5cm);
%     \coordinate (C) at (0:2.25cm);
%     \draw[line width = 0.05cm] (A)--(C)--(B)--cycle;
%     }}}


\newcommand{\rtrapskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.3 with {\arrow[scale=2.75]{<}},
          mark=at position 0.55 with {\arrow[scale=2.75]{<}},
          mark=at position 0.8 with {\arrow[scale=2.75]{<}}
        }]
        \coordinate (A) at (0,0);
        \coordinate (B) at (90:1.5cm);
        \coordinate (C) at (2.5,0.9cm);
        \coordinate (D) at (0:2.5cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
      \end{scope}
    }}}

\newcommand{\rtriskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.7 with {\arrow[scale=2.75]{|}}
        }]
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
      \end{scope}
    }}}

\newcommand{\rtri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\newcommand{\ltri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\date{}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}



\section{Introduction}


% There has been a surge of recent work on dep parsing

There are two dominant grammatical
frameworks used for statistical syntactic parsing: phrase-structure and
dependency parsing \cite{}. The two often offer a practical
trade-off. Phrase-structure parsing is very accurate and provides a full
context-free grammatical representation; while dependency parsing is
much faster, both asymptotically and empirically, while still
predicting much of the important structural relationships in a
sentence.

Since statistical phrase-structure parsers often use an internal
lexicalized representation, it is possible to directly compare the
performance of the two models in terms of dependency accuracy.
\cite{kong2014empirical} run this comparison across a wide-range of
popular freely available parsing models and find that while this
trade-off is still true, the gap in accuracy has significantly
narrowed over the last several years. State-of-art dependency parsers
such as TurboParser \cite{}, and \cite, perform only slightly worse in
terms of accuracy than large-scale reranking dependency parsers, while
still maintaining efficient run-time.

\begin{figure}
  \centering
  \vspace{-1cm}

  \begin{dependency}[theme=simple]
    \begin{deptext}[column sep=0.7cm]
      I$_1$ \& saw$_2$ \& the$_3$ \& man$_4$ \\
    \end{deptext}
    \deproot{2}{ROOT}
      \depedge{2}{1}{}
      \depedge{3}{4}{}
      \depedge{2}{4}{}
  \end{dependency}

  \scalebox{0.7}{
    \Tree [ .X(2) [ .X(1) [ .N I$_1$ ] ]  [ .V saw$_2$ ] [ .X(4) [ .D the$_3$ ]  [ .N man$_4$ ] ] ]
    \Tree [ .X(2)  [ .N I$_1$ ] [ .X(2)  [ .V saw$_2$ ] [ .X(4) [ .D the$_3$ ]  [ .N man$_4$ ] ] ] ]
    \Tree [ .X(2) [ .X(1) [ .N I$_1$ ]   [ .V saw$_2$ ] ]  [ .X [ .D the$_3$ ]  [ .N man$_4$ ] ] ]
  }
  \label{fig:inverse}
  \caption{While an LCFG parse determines a unique dependency parse, the inverse problem is non-deterministic. The figure, adapted from \cite{collins1999statistical}, shows several LCFG parse structures that all correspond to the dependency structure shown. The parentheses $X(h)$ indicate the head $h$ of each internal vertex. }
\end{figure}


Given these improvements, we ask the natural reverse question. How
much of the phrase-structure is recoverable from the dependency
representation?  While the transformation from phrase-structure to
dependencies is deterministic, the inverse is not (see
Figure~\ref{}). Can we learn to invert this process and recover the
richer phrase-structure trees from the dependency representation?

% However, while this conversion is deterministic from lexicalized context-free grammars
% to dependency parses, the inverse problem is more difficult. For a given dependency
% graph there may be many different valid context-free parses. This problem is the focus of this work.



This conversion is challenging in two ways:
(1) The original dependency parse does not contain any information about the symbols at each vertex. From the context we need to
determine the nonterminals $A$ and the rules used. (2)  The dependency structure does not specify the shape of the context-free parse. Even without syntactic symbols many
context-free trees may be valid for a single dependency parse. This is shown in Figure~\ref{fig:inverse}.
\end{itemize}


Because of these issues is not obvious how to directly perform this conversion without doing search over possible structures.



\todo[inline]{name?}

% \textsc{Miniparser}
To approach this question we present a very simple statistical parser. The parser is a complete exact
lexicalized parser; except that unlike standard parsers, it takes both
a sentence and a dependency tree as input. Using this non-standard setup
has several advantages:

\begin{itemize}
\item The parser is asymptotically three-orders of magnitude faster than
standard phrase-structure parsers and practically as efficient as the fastest
high-accuracy dependency parsers.

\item Despite being constrained to pre-selected dependency decisions,
the parser is comparably accurate to non-reranked phrase-structure parsers.

\item The framework can be used...

% \cite{}
% \cite{}
% \cite{}

\end{itemize}






% In this paper we ask a simple question: can we invert this process?
% Given the raw skeleton of a dependency parse, is it plausible to obtain
% an accurate phrase-structure parse for a sentence.





% This work has brought the accuracy of dependency parsing much closer to the level
% of powerful phrase- parser. For instance the parser of \cite{}
% score \% on dependency UAS whereas the has UAS etc.



There has been a series of work that looks at the relationship
between dependency and constituency parsing. \cite{carreras2008tag}
build a powerful dependency parsing-like model that predicts
adjunctions of the elementary TAG spines. \cite{rush2010dual} use
dual decomposition to combine a powerful dependency parser with
a simple constituency model.

There have also been several papers that take the idea of
shift-reduce parsing, popular from dependency parsing, and apply
 it constituency parsing. \cite{zhu2013fast} produces a state-of-the-art shift-reduce
system.

Our work is differs in that it look at a more stripped-down problem.
We give up entirely on determining the structure of the tree itself and only
look at  producing the nonterminals of the tree.

% \section{Overview}

% In recent years there has been a steady progression in the speed and accuracy of dependency parsers \cite{}; however
% there are still many tasks that rely heavily on the full context-free parses \cite{}. It is therefore desirable
% to produce context-free parses directly from dependency structures.




% \section{Overview}

% The parser
\section{Background}

We begin by developing notation for a lexicalized phrase-structure formalism and for dependency parsing. The notation aims to highlight the similarity between the two formalisms.

\subsection{Lexicalized CFG Parsing}

A lexicalized context-free grammar (LCFG) is an extension to a standard context-free grammar where each vertex also is annotated with a lexical head. Define an LCFG as a 4-tuple $(\nonterms, \rules, \terms, r)$ where:
\begin{itemize}
\item $\nonterms$; a set of nonterminal symbols, e.g. \Tag{NP}, \Tag{VP}.
\item $\terms$; a set of terminal symbols, often consisting of the words in the language.
\item $\rules$; a set of lexicalized rule productions of the form $\langle A \rightarrow \beta_1 \ldots \beta_m, k\rangle$ consisting of a parent nonterminal $A \in \nonterms$, a sequence of children $\beta_i \in \nonterms \cup \terms$ for $i \in \{1\ldots m\}$, and a distinguished head child $\beta_k$. The value $k$ comes from the head rules associated with the grammar.
\item $\Root$; a distinguished root symbol $\Root \in \nonterms$.
\end{itemize}

Given an input sentence $x_1, \ldots, x_n$ consisting of terminal symbols from $\terms$, define $\mathcal{Y}(x)$ as the set of valid lexicalized parses for the sentence. Informally this set consists of all ordered trees with fringe $x_1, \ldots,  x_n$, internal nodes labeled from $\nonterms$, all tree productions  $A \rightarrow \beta$ consisting of members of $\rules$, and root label $\Root$.


% Given fixed sequence of terminal symbols $x_1 \in \terms, \ldots, x_n \in \terms$, for instance a sentence of words, a context-free parse consists of any ordered, finite-size tree where the interior labels come $\nonterms$, the fringe labels correspond to the initial sequence, and all tree productions

For a parse of a sentence $y \in \mathcal{Y}(x)$,
we further associate a triple $v = (\Span{i, j}, h, A)$ with each vertex in the tree, where
% We define the following important properties of this parse tree for each vertex:


\begin{itemize}
\item $\Span{i,j}$; the \textit{span}  of the vertex, i.e. the contiguous sequence $\{x_i, \ldots, x_j\}$ of the sentence covered by the vertex.

\item $h \in \{1, \ldots n\}$; index indicating that $x_h$ is the \textit{head} of the vertex, defined recursively by the following rules:
  \begin{enumerate}
  \item  If the vertex is leaf $x_i$, then $h=i$.
  \item Otherwise,  $h$ matches the head of the $k$'th child of the vertex where $\langle A \rightarrow \beta_1 \ldots \beta_m, k\rangle$  is the rule production at this vertex.
  \end{enumerate}
  Let $h(v)$ indicate the head of vertex $v$.
\item $A \in \terms \cup \nonterms$; the terminal or nonterminal symbol of the vertex.
\end{itemize}

Note that each word $x_i$ is the head of a fringe vertex of the tree,
and at some ancestor vertex ceases to play this role.  Define the
\textit{spine} of word $x_i$ to be the longest chain connected vertices $v_1,
\ldots, v_p$ where $h(v_j) = i$ for $j \in \{1, \ldots, p\}$.
Also if it exists, let vertex $v_{p+1}$  be the parent of vertex $v_p$,
where $h(v_{p+1}) \neq i$. The full notation is illustrated in Figure~\ref{fig:spine}.

\begin{figure}
  \centering

  
  \Tree [ .\node[color=red]{$(\Span{1,n}, 3, \Tag{S})$}; [ .\node[color=blue]{$(\Span{1,2}, 2,  \Tag{NP})$}; [  .\node{$(\Span{1,1}, 1,  \Tag{DT})$}; The$_1$ ]  [ .\node[color=blue]{$(\Span{2,2}, 2, \Tag{NN})$}; \textcolor{blue}{automaker$_2$} ] ] [ .\node{$(\Span{3,3}, 3,  \Tag{VBD})$}; sold$_3$ ] [ .\node{..}; ] ]



  \begin{dependency}[theme=simple]
    \begin{deptext}[column sep=0.7cm]
      The$_1$ \& automaker$_2$ \& sold$_3$ \& $\ldots$ \\
    \end{deptext}
    \deproot{3}{}
      \depedge{2}{1}{}
      \depedge{3}{2}{}
  \end{dependency}


  % \Tree [ .A [ .B  C ] ]
  \caption{Figure illustrating an LCFG parse. The parse is an ordered tree with fringe $x_1, \ldots, x_n$. Each vertex is annotated with a span, head, and syntactic tag. The blue vertices represent the 3-vertex spine $v_1, v_2, v_3$ of the word \texttt{automaker$_2$}. The root vertex is $v_4$, which implies that \texttt{automaker$_2$} modifies \texttt{sold$_3$} in the induced dependency graph.     }
  \label{fig:spine}  
\end{figure}

\subsection{Dependency Parsing}

Dependency trees provide an alternative, and in some sense simpler,
representation of grammatical structure.

Given an input sentence $x_1
\ldots x_n$, we allow each word to modify another word,  and define these dependency decisions as a sequence $d_1 \ldots d_n$ where for all $i$, $d_i \in \{0, \ldots, n\}$ and $0$ is a pseudo-root position. These dependency relations can be seen as arcs $(d_i, i)$ in a directed graph. (as
shown in Figure~\ref{fig:spine}). A dependency parse is valid  if the corresponding directed graph is a directed tree rooted at
vertex $0$.
% to be the head of the sentence, i.e. it modifies $g_i \in \{0, n\}$
% where $0$ is a special pseudo-root symbol.
% If each word and the pseudo-root is represented as a vertex, than
% these relations correspond to arcs $(g_i, i)$ in a directed graph

Given a valid dependency parse, we can define the span of any word $x_m$ as the set of indices reachable from vertex $m$ in the directed tree. A dependency parse is \textit{projective} if the descendants of every word in the tree form a contiguous span of the original sentence \cite{}. For we use the notation $\Left{m}$ and $\Right{m}$ to represent the left- and
right-boundaries of this span.

The highest-scoring projective dependency parse under an arc-factored scoring function can be found in time $O(n^3)$.





% define a sequence of dependencies $()$

% define a sequence $g_1 \ldots
% g_n$ where for all $i$, $t_i$ is the dependent of position $g_i \in  \{0,
% \ldots n\}$, and $0$ is a special pseudo-root symbol.

We now give the important property for this work.

Any lexicalized context-free parse, as defined above, can be converted
deterministically into a projective dependency tree.


For an input symbol $x_m$ with spine $v_1, \ldots, v_p$,

\begin{enumerate}
\item If $v_p$ is the root of the tree,
then $d_m = 0$.
\item Otherwise let $v_{p+1}$ be the parent vertex of
$v_p$ and $d_m = h(v_{p+1})$. The span $\Span{i, j}$ of $v_p$ in the lexicalized parse is equivalent to $\Span{\Left{m}, \Right{m}}$
in the induced dependency parse.
\end{enumerate}

% In the binarized case,


The conversion produces a directed tree rooted at $0$, by preserving the tree structure of the original LCFG parse.




% Any lexicalized







% For a specific parse $p$ the head rules induce a projective dependency parse over the sentence.
% To see this, as a base case associate a head word $i$ with each node $(i, i)$ in the tree.
% Then inductively at each inner node with span $(i,j)$ assign the head word $i \leq h \leq$ where
% $h$ is the head word of the $H(r)$th child node.

% The span of a word $i$ is the span $(i, j)$ of $p_n$.


% Define the spine of a word $i$ as the sequence of nodes $p_1, \ldots p_n$ such that $h(p_j) = i$.
% Let $p_{n+1}$ be the parent of $p_n$, then we say that $i$ is the dependent of $h(p_{n+1})$. By convention
% if $p_n$ is the root of the tree then $i$ is the dependent of the pseudo-root $0$.

% We can think of these dependencies as a directed graph over the nodes $\{0, \ldots n\}$ where the arc $(j, i)$ indicates that
% word $i$ is a dependent of word $j$.


% $l,r:\{1\ldots n\} \mapsto \{1\ldots n\}$


% While we have described dependencies as the deterministic by-product
% of phrase-structure parsing, in recent years there has been extensive
% work on directly predicting the dependency structure $(j,i)$ without
% predicting the full parse structure. The main benefit of this approach
% is that these structures can be predicted efficiently which has allowed
% for extensive use of powerful structured prediction techniques.






% A labeled dependency parse is one with a set $\mathcal{L}$
% and $(i, j, l)$.


% \subsection{Dependency Parsing}


% A dependency parse is any directed tree over this graph rooted
% at node $0$.


% If there exists a path from $w_i$ to $w_j$ than $w_j$ is a
% an ancestor of word $i$.

% Define a \textit{projective} dependency parse as one where for any word
% the ancestors are contiguous.

% We can specify a dependency parse as a sequence $(i,j)$.


\section{Parsing Dependencies}

To solve this inverse problem we set up as simple variant of the
original lexicalized parsing algorithm. While this algorithm has the
same form of the original, we show that it can be solved
asymptotically faster.




\subsection{Constrained Lexicalized CKY}

We will assume in this section that the LCFG is in lexicalized Chomsky normal form, 
with all non-preterminal rules of the form $\Rule{A}{\beta_1}{\beta_2}$. 
In the next section we show how to convert our original grammar to this form while 
preserving the head structure. 

A standard parsing  algorithm for a grammar in this form is the lexicalized
variant CKY algorithm. This algorithm can be represented as a
collection of inductive rule productions as shown in
Figure~\ref{fig:cky}. These rules compactly encode the set of all 
valid parses for a given sentence under the grammar. For example consider the rule

\[ \infer{(\Span{i, j}, h, A)}{(\Span{i, k}, m, \beta_1) &  (\Span{k+1, j}, h, \beta_2)} \]
 for all rules $\RuleA{A}{\beta_1}{\beta_2}\in \rules$ and spans $i \leq k < j$. This rule indicates that grammatical rule $\RuleA{A}{\beta_1}{\beta_2}$  was applied at a vertex covering $\Span{i, j}$ to produce two vertices covering $\Span{i, k}$ and $\Span{k+1, j}$.





% We abbreviate this decision as a tuple $(\Span{i ,k, j}, h, m, r)$, and for $y \in \mathcal{Y}$

% define  $y$ as an indicator vector over decisions i.e.  $(\Span{i ,k, j}, h, m, r) = 1$ to indicate that the rule above was used.


% Recall that the each vertex in the tree has the form $(\Span{i, j}, h,
% A)$. After binarization, each tree production is roughly of the form
% $(\Span{i, j}, h, A) \rightarrow (\Span{i, k}, m, \beta_1)\ (\Span{k+1, j}, h, \beta_2)$ for $A \in \nonterms, B, C \in \nonterms \cup \terms, i < k < j,$ and  .
% For notational simplicity, we write this production
% where $r = \langle A \rightarrow B\ C, 1\rangle $ is the rule applied.


% To begin we consider parsing a standard lexicalized context-free grammar with a
% factored scoring function.

Define this complete set of valid parses for a sentence as  $\mathcal{Y}(x)$.
The lexicalized parsing problem is to find the highest-scoring parse in this set, i.e.  \[ \hat{y} \gets \argmax_{y \in {\cal Y}(x)} s(y;x) \] where
$s$ is a scoring function that factors over each logical production.


The highest-scoring parser can found by bottom-up dynamic programming
over this set. The running time of this algorithm is linear 
in the number of rules. Since five free indices $i, j, k, h, m$ and
$|\rules|$ possible grammatical rules, the algorithm
requires $O(n^5 |\rules|)$ time.

This running-time makes this algorithm highly impractical to run on
real data without heavy pruning. 

However we are interested in a
highly-constrained version of this problem. Assume that we are additionally given a projective dependency parse $d_1, \ldots, d_n$.
Define the set $\mathcal{Y}(x,d)$ as all valid LCFG parses that match
this dependency parse 

\[
  {\cal Y}(x,d )  = \{ y \in {\cal Y}(x): \mathrm{for\ all\ }(h,m) \in y,  d_m =  h   \}
\]



% Our contribution to this problem will

% In this section we first introduce a binarization approach for lexicalized context free

% For any parse produced in the new grammar, we can convert it to a tree
% in the original grammar with the same induced dependency tree.

% \todo[inline]{check this}



% However doing it for
% all parse structures yields a $O(n^5)$ running time algorithm.


% Given an instance of the problem $x$ consisting of a sentence, part-of-speech tags,
% and a predicted dependency parse.

% Let the set $\mathcal{Y}(x)$ represent all possible parse trees for a sentence.
% Define the set $\mathcal{Y}(x)$ to contain all tree that agree with the dependency parse structure given.
% That is if rule $y \in \mathcal{Y}$ and  $(i, j, k, h, m) \in y$ then $(h, m) \in x$.

% Our goal will be to solve the optimization $\argmax_{y \in \mathcal{Y}} w^{\top} y$ for some any vector $w$

% The standard algorithm for binarized context-free grammar is known as the CKY algorithm. For review
% the inductive form of the CKY algorithm is shown in Figure~\ref{fig:cky}.


% \subsection{Parsing Dependencies}

\noindent and our aim is to find

\[ \argmax_{y \in \mathcal{Y}(x, d)} s(y; x, d)\]

Now consider the following simple property of this new problem. For any word $x_m$ with spine $v_1 \ldots v_p$ the LCFG span  $\Span{i,j}$ of $v_p$ is equal to the dependency span $\langle \Left{m},\Right{m}\rangle$ of $x_m$. Furthermore these spans can be easily computed directly from $d$.

Using this property, we can greatly limit the search space of the original problem.
Instead of searching over all possible spans $\Span{i, j}$ of each modifier, we can
precompute $\langle \Left{m},\Right{m}\rangle$.  

Figure~\ref{fig:cky_new} shows the new inductive rules which are
virtually identical to the original lexicalized algorithm.
In fact, since there are $n$ dependency links $(h, m)$ and $n$ indices $i$ the new algorithm has $O(n^2|\rules|)$ running time.



% Now we make a simple extension to this algorithm. In the last section $x$ was just the sentence, now we assume that we also are given the dependency parse. Define ${\cal Y}(x)$ to the be the set of parse tree that agree with the dependency structure.


% That is if a rule in the parse $y \in \mathcal{Y}$ then the implied dependency is $(h, m)$ is in the dependency parse. The new problem is to solve

% \[ \argmax_{y \in {\cal Y}(x)} s(y;x) \]

% 
% Recall that $\Left{m}$ and $\Right{m}$ are the precomputed left- and right- boundary words of the cover of word $m$.

% While the algorithm is virtually identical to the standard lexicalized CKY algorithm, we are now able to fix many of the free indices. 

% \begin{figure}
%   \noindent \textbf{Premise:}
%   \[(\Span{i, i}, i, A)\ \ \ \forall i \in \{1 \ldots n\}, A \in {\cal N}\]

%   \noindent\textbf{Rules:}


%    For $i\leq m \leq k < h \leq j$,  and  rule  $\langle A \rightarrow \beta_1\ \beta_2, 1\rangle$,
%    \[\infer{( \Span{i, j},  h,  A)}{( \Span{i, k}, h, \beta_1)  &  ( \Span{k+1, j}, m, \beta_2) } \]

%    For $i\leq h \leq k < m \leq j$, rule  $\langle A \rightarrow \beta_1\ \beta_2,  2\rangle$,
%    \[\infer{(\Span{i, j},  h, A)}{(\Span{i, k}, m, \beta_1)  &  (\Span{k+1, j}, h, \beta_2) }  \]

% \noindent \textbf{Goal:}\[ (\Span{1, n}, m, \Root) \mathrm{\ for\ any }\ m\]
% \label{fig:cky}
% \caption{Standard CKY algorithm for LCFG parsing stated as inductive rules. Starting from the \textit{premise}, any valid application of \textit{rules} that leads to a \textit{goal} is a valid parse. Finding the optimal parse with dynamic programming is linear in the number of rules. For this algorithm there are $O(n^5|\rules|)$ rules where $n$ is the length of the sentence.}

% \end{figure}

\begin{figure}
  \noindent \textbf{Premise:}
  \[(\langle i,i \rangle, i, A)\ \ \ \forall i \in \{1 \ldots n\}, A \in {\cal N}\]

  \noindent\textbf{Rules:}

   % For all    $h$,  nonterminals $A \in \nonterms$,
   % \[ \infer{( \tri, h, A)}{(\Span{\Left{h}, \Right{h}}, h, A) } \]


   For all   $i < m, h = d_m$  and rule  $\RuleA{A}{\beta_1}{\beta_2}$,
  \[\infer{( \Span{ i, \Right{m} }, h, A)}{(\Span{i, \Left{m}-1}, h, \beta_1)  &  (\tri, m, \beta_2) } \]

  For all    $m < j, h = d_m$ and  rule  $\RuleB{A}{\beta_1}{\beta_2}$,
  \[ \infer{( \Span{\Left{m}, j}, h, A)}{(\tri, m, \beta_1)  &  (\Span{\Right{m}+ 1, j}, h, \beta_2) } \]


% \forall  i, j, h, A \rightarrow B^* \\\\
%   \infer{(\Span{i, j}, h, A)}{(\Span{i, j}, h, \beta_1)}  \\\
% \\

\noindent \textbf{Goal:}\[ (\Span{1, n}, m, \Root) \mathrm{\ for\ any }\ m \mathrm{\ s.t. \ } d_m = 0  \]
\label{fig:cky_new}
\caption{The constrained CKY parsing algorithm for $\mathcal{Y}(x, d)$. The algorithm is nearly identical to Figure~\ref{} except that many of the free indices are now fixed to the dependency parse. Finding the optimal parse is now $O(n^2|\rules|)$}.
\end{figure}

% Here we assume there can




\subsection{Extension: Labels}

\todo[inline]{Finish this section}

Standard dependency parsers also predict labels from a set ${\cal L}$ on each dependency link.
In a labeled dependency parser a would be of the form $(i, j, l)$.

This label information can be used to encode further information about the parse structure. For instance
if we use the label set ${\cal L} = \nonterms \times \nonterms \times \nonterms$, encoding the binary rule decisions $\Rule{A}{\beta_1}{\beta_2}$.


\subsection{Binarization}

While the algorithm itself is not dependent on the LCFG binarization
method used, the choice of binarization effects the run-time of the
algorithm, through $\rules$, as well as the modeling accuracy, through
the factored scoring function $s$.

% A general LCFG can be binarized while still maintaining the lexical
% structure and making it possible to recover the original tree. 
Since the parser traces a fixed dependency structure
we select a binarization based around the head 
structure.

For simplicity, we consider binarizing rule $\langle A \rightarrow \beta_1 \ldots \beta_m,
k\rangle$ with $m > 2$. Relative to the head $\beta_k$
the rule has left-side $\beta_1 \ldots \beta_{k-1}$ and right-side
$\beta_{k+1} \ldots \beta_m$. 

We replace this rule with binary rules that consume each side
independently as a first-order Markov chain (horizontal Markovization). 
The main transformation is to introduce rules

\begin{itemize}

\item
$\RuleA{A^{\Rightarrow}_{\beta_i}}{A^{\Rightarrow}_{\beta_{i-1}}}{\beta_i}$ for $k > i > m$

\item
$\RuleB{A^{\Leftarrow}_{\beta_i}} {\beta_i}{A^{\Leftarrow}_{\beta_{i+1}}} $ for $1< i < k$
\end{itemize}  

Additionally we introduce several additional rules to handle the boundary cases of starting a new rule, finishing the right side, and completing a rule. (These rules are slightly modified when $k=1$ or $k=m$) .

\begin{itemize}

\item $\RuleA{A^{\Rightarrow}_{\beta_{k+1}}}{\beta_k}{\beta_{k+1}} $

\item 
$\RuleA{A^{\Rightarrow}_{\END}} {A^{\Rightarrow}_{\beta_{m-1}}}{\beta_m}$

\item  $\RuleB{ A^{\Leftarrow}_{\beta_{k-1}}}{\beta_{k-1}}{A^{\Rightarrow}_{\END}}$

\item
$\RuleB{A}{\beta_1}{ A^{\Leftarrow}_{\beta_{2}}} $ 
\end{itemize}

\noindent Each rule contains at most 3 nonterminals so the size of the new binarized rule set is bounded by $O(\nonterms^3)$.    


% \subsection{Complexity Analysis}

% Since we know the dep tree, there are exactly $n$ $h \rightarrow m$ arcs. Therefore there are $O(n^2 |{\cal R}|)$ binary terms.


% The unary terms are more difficult to limit. Worst case it will have $O(n^3 |{\cal R}_1|)$ terms where ${\cal R}_1$ is the set of unary rules.
% In practice there will be $O(n \times \max_h({\#l_h \times  \#r_h}) \times |{\cal R}_1|)$ terms where $\#r_h$ and $\#l_h$ are the number of left and right modifiers
% of $h$ respectively.

\section{Structured Prediction}

To learn the transformation from dependency trees to phrase-structure trees, 
we use a standard structured prediction setup. 
The goal is to predict the highest-scoring LCFG parse for sentence and dependency tree. 
We define the scoring function $s$ as

\[s(y;x, d, \theta) =  \theta^{\top} f(x, d, y) \]

\noindent
where $\theta \in \Reals^D $ is a learned weight vector that will be trained and $f(x, d, y)$ is a feature function that maps parse production (as in Figure~\ref{}) to feature vectors in $\{0,1\}^D$ . In this section we first discuss the features used and then how to  estimate the parameter vector $\theta$.


\subsection{Features}

% We model this problem using  a standard structured prediction set-up.



% \[\hat{y} \gets \argmax_{y\in \mathcal{Y}(x)}  s(y; x) \]

% where the function $f$ is a feature function that maps possible parse structures to
%   a sparse feature vector in $\{0,1\}^d$. For efficiency we assume that $f$
% factors as a sum of parts

% \[ f(x, y) = \sum_{(i, j, k, h, m, r) \in y} f(\langle i,j,k,h,m,r\rangle,  x) \]


We experimented with several settings of the part features.
The final features are shown in  Figure~\ref{fig:features}.

The first set of features look at the rule structure $\Rule{A}{\beta_1}{\beta_2}$.
We have features on several combinations of rule and non-terminals.

The next set of features look at properties of the lexicalization.
These look at combinations of the head word and tags.

The final set of features looks at more structural properties.


\todo[inline]{bug lp about this.}
% We have features on several combinations of rule and non-terminals.


\cite{}
\cite{}
\cite{}



\begin{figure}
  \centering
  For a part $(\span{i, j, k}, h, m, \Rule{A}{\beta_1}{\beta_2})$

  \vspace{0.5cm}


  \begin{tabular}{|l|l}

  \hline
  Nonterm Features \\
  \hline

  \hline
  $(A$, $\beta_1)$ \\
  $(A$, $\beta_2)$ \\

  $(A, \TagFN{h}, \TagFN{m})$ \\
  $(A, B, \TagFN{m})$ \\
  $(A, C, \TagFN{h})$ \\
  $(A, \TagFN{h})$ \\
  $(A, \WordFN{h})$ \\
  \hline
  \hline


  Rule Features \\
  \hline

  \hline
  $(\Rule{A}{\beta_1}{\beta_2}  )$\\
  $(\Rule{A}{\beta_1}{\beta_2}, \TagFN{h}, \TagFN{m})$ \\
  $(\Rule{A}{\beta_1}{\beta_2}, \WordFN{h}, \TagFN{m})$ \\
  $(\Rule{A}{\beta_1}{\beta_2}, \WordFN{h})$ \\
  $(\Rule{A}{\beta_1}{\beta_2}, \TagFN{h})$ \\
  $(\Rule{A}{\beta_1}{\beta_2}, \WordFN{m})$ \\
  $(\Rule{A}{\beta_1}{\beta_2}, \TagFN{m})$ \\

  shape\\
  \hline
  \end{tabular}

  \label{fig:features}
  \caption{The templates used in the structured model. The functions $\TagFN(i)$ and $\WordFN(i)$ give the part-of-speech tag and word at position $i$.
  }
\end{figure}

\subsection{Training}

We train the parameters $\theta$ using a variant of standard structured SVM training \cite{}.

We assume that we are given a set of gold-annotated parse examples: $( x^{1}, y^{1}), \ldots,  (x^{D}, y^{D})$. We also define $d^{(1)} \ldots d^{(D)}$ as the dependency structures induced from $y^{1} \ldots y^{D}$


Our aim to find parameters that satisfy the following regularized risk minimization

\[ \min_{\theta} \sum_{i = 1}^D \max\{0,  \ell( x^{i}, d^{i} , y^{i}, \theta) \} + \frac{\lambda}{2} ||\theta||^2_2 \]

\noindent and define

\[\ell(x, d, y, \theta) = s(y) + \max_{y' \in \mathcal{Y}(x, d)}\left(s(y')  + \Delta(y, y') \right) \]


\noindent where $\Delta$ is a problem specific cost-function that we assume is linear in either arguments.


We consider two extensions to the standard setup.

First is that at training time we do not have access to the true dependency structure $d$, but instead only predicted dependencies $\hat{d}$. Past work on parsing  \cite{} has shown that it can be important to train using these predicted structures.

However, using $\hat{d}$ may make it impossible to recover the gold LCFG parse $y$, e.g. $y \not \in \mathcal{Y}(x, \hat{d})$. To handle this issue, we also replace $y$ with the oracle parse $\bar{y}$ defined at the projection of $y$ onto $\mathcal{Y}(x, \hat{d})$

\[ \bar{y} \gets \argmin_{y' \in \mathcal{Y}(x, \hat{d})} \Delta(y', y) \]

If $\Delta$ is linear in its first argument, this projection can be found by running the same algorithm as in Figure~\ref{}.

We replace the loss with $\ell(x^{(i)}, \hat{d}^{(i)}, \bar{y}^{(i)})

% \[l(i, \theta) = s(\bar{y}^{(i)}) + \max_{y \in \mathcal{Y}(x^{(i)}, \hat{d}^{(i)})}\left( s(y)  + \Delta(\bar{y}^{(i)}, y) \right) \]

In our experiments, we use a simple hamming loss $ \Delta(y, \bar{y}) = || y - \bar{y}||$.



At training time, we run 10-fold jack-knifing to produce dependency parses $\hat{d}$. We then run a single pass to calculate $\bar{y}$ for each training example.

We then optimize using stochastic gradient descent \cite{}. The gradient requires calculating a loss-augmented argmax for each training example.


% One issue with directly minimizing this objective, is that

%  for this problem is the role that
% parsing issues play. If there is a dependency parsing mistake makes it often impossible
% for the system to recover the correct parse.


% Define a loss function $ \Delta(\hat{y}, y)$ to determine the difference between two parses.
% We assume that  the loss function is linear in its first argument.




% Define the oracle parse to be the parse minimizes the loss function


% Since this functions is linear we can use our

% To learn the weight vector $\theta$ we optimize a standard structured svm objective




% Note that we are optimizing towards the oracle parse.


\section{Data and Setup}


\subsection{Data}

We used wsj..

We used ctb 5-1..




\subsection{Implementation}

We built...

Parser is in C++, publicly available, 500 lines of code..


\subsection{Binarization}

Before describing our parsing algorithm we first describe a binarization approach to make efficient parsing possible and highlight the relationship between the LCFG and the dependency parse.

% We can binarize a context-free grammar to produce a new grammar where for all rules $A \rightarrow \beta$ the length of $\beta$ is in either $\{1, 2\}$
% in such a way that we can obtain the original tree.

% We first describe our binarization approach. Our aim is for every non-unary rule to correspond to exactly one dependency arc.


\subsection{Extension: Pruning}

We also experiment with a simple pruning dictionary pruning technique.
For each context-free rule $A \rightarrow \beta_1\ \beta_2$ and POS tag $a$ we
remove all rules that were not seen with that tag as a head in training.



\subsection{}

\section{Results}


\subsection{}


\begin{table*}
  \centering
  Parsing Results


  \begin{tabular}{l|lllllll}
    \hline
    & \multicolumn{7}{|c}{wsj} \\
    \hline
    & speed & fscore \\
    \hline
    Petrov & & & \\
    Carreras & & & \\
    \hline
    \hline
    & \multicolumn{7}{|c}{ctb} \\
    \hline
  \end{tabular}
  \caption{This is the big monster result table that should tower above all comers. }
\end{table*}

\begin{table}
  \centering
  \begin{tabular}{l|ll}
    Model & fscore & speed  \\
    \hline
    \textsc{TurboParser} & & \\
    \textsc{MaltParser} & & \\
    \textsc{EasyFirst} & & \\
  \end{tabular}
  \caption{This }
\end{table}

\textbf{Acknowledgment} sections should go as a last (unnumbered) section immediately
before the references.

\bibliography{full}
\bibliographystyle{naaclhlt2015}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End
