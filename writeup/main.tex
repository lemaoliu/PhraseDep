%
% File naaclhlt2015.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{proof}
\usepackage{fullpage}
\usepackage{todonotes}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}


\usepackage{amsfonts}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{algpseudocode}
\usepackage{algorithm}

\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}
\tikzset{
  >=latex,text height=1.5ex,text depth=0.25ex
}

\newcommand{\nonterms}{\mathcal{N}}
\newcommand{\rules}{\mathcal{R}}
\newcommand{\terms}{\mathcal{T}}
\newcommand{\Left}[1]{#1_{\Leftarrow}}
\newcommand{\Right}[1]{#1_{\Rightarrow}}
\newcommand{\Span}[1]{\langle #1 \rangle}
% \newcommand{\root}{r}

\newcommand{\Tag}[1]{\texttt{#1}}
\newcommand{\Root}{root}


\newcommand{\Reals}{\mathbb{R}}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Parsing Dependencies}

\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}


%% For drawing the traps/tris.

\newcommand{\abox}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (0,1.5cm);
    \coordinate (C) at (-2cm, 1.5cm);
    \coordinate (D) at (-2cm, 0cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\rtrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (2.5,0.9cm);
    \coordinate (D) at (0:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}


\newcommand{\ltrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (-2.5,0.9cm);
    \coordinate (D) at (180:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\tri}{\Span{\Left{m}, \Right{m}}}

% \newcommand{\tri}{\scalebox{0.2}{\tikz{
%     \coordinate (A) at (0,0);
%     \coordinate (B) at (45:1.5cm);
%     \coordinate (C) at (0:2.25cm);
%     \draw[line width = 0.05cm] (A)--(C)--(B)--cycle;
%     }}}


\newcommand{\rtrapskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.3 with {\arrow[scale=2.75]{<}},
          mark=at position 0.55 with {\arrow[scale=2.75]{<}},
          mark=at position 0.8 with {\arrow[scale=2.75]{<}}
        }]
        \coordinate (A) at (0,0);
        \coordinate (B) at (90:1.5cm);
        \coordinate (C) at (2.5,0.9cm);
        \coordinate (D) at (0:2.5cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
      \end{scope}
    }}}

\newcommand{\rtriskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.7 with {\arrow[scale=2.75]{|}}
        }]
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
      \end{scope}
    }}}

\newcommand{\rtri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\newcommand{\ltri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\date{}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}





\section{Introduction}


There has been a surge of recent work on dep parsing

\cite{}
\cite{}
\cite{}

This work has brought the accuracy of dependency parsing much closer to the level
of powerful constituency parser. For instance the parser of \cite{}
score \% on dependency UAS whereas the has UAS etc.


In this paper we ask a simple question: can we invert this process?
Given the raw skeleton of a dependency parse, is it plausible to obtain
an accurate phrase-structure parse for a sentence.

To approach this question we build a very simple parser,
that takes in the fully specified


There has been a series of work that looks at the relationship
between dependency and constituency parsing. \cite{carraers}
build a powerful dependency parsing-like model that predicts
adjunctions of the elementary TAG spines. \cite{rush} use
dual decomposition to combine a powerful dependency parser with
a simple constituency model.

There have also been several papers that take the idea of
shift-reduce parsing, popular from dependency parsing, and apply
 it constituency parsing. \cite{zpar} produces a state-of-the-art shift-reduce
system. \cite{stanford}

Our work is differs in that it takes on a much more stripped down problem.
We give up entirely on determining the structure of the tree itself and only
look at  producing the nonterminals of the tree.

% \section{Overview}

% The parser
\section{Preliminaries}

We begin by introducing notation for two standard grammatical structures: lexicalized context-free parses and dependency parses. The notation emphasizes the similarity between the two formalisms.

\subsection{Lexicalized CFG Parsing}

Define a lexicalized context-free grammar (LCFG) as a 4-tuple $(\nonterms, \rules, \terms, r)$ where:
\begin{itemize}
\item $\nonterms$; a set of nonterminal symbols, e.g. \Tag{NP}, \Tag{VP}.
\item $\terms$; a set of terminal symbols, often consisting of the words in the language.
\item $\rules$; a set of lexicalized rule productions of the form $\langle A \rightarrow \beta_1 \ldots \beta_m, k\rangle$ consisting of a parent nonterminal $A \in \nonterms$, a sequence of children $\beta_i \in \nonterms \cup \terms$ for $i \in \{1\ldots m\}$, and a distinguished head child $\beta_k$.
\item $\Root$; a distinguished root symbol $\Root \in \nonterms$.
\end{itemize}

For a given input sequence $x_1, \ldots, x_n$ consisting of terminal symbols from $\terms$, define $\mathcal{Y}(x)$ as the set of valid lexicalized parses for the sentence. Informally this set consists of all ordered trees with fringe $x$, internal nodes labeled from $\nonterms$, all tree productions  $A \rightarrow \beta$ consisting of members of $\rules$, and root label $\Root$.


% Given fixed sequence of terminal symbols $x_1 \in \terms, \ldots, x_n \in \terms$, for instance a sentence of words, a context-free parse consists of any ordered, finite-size tree where the interior labels come $\nonterms$, the fringe labels correspond to the initial sequence, and all tree productions

For a given parse $y \in \mathcal{Y}(x)$,
we further associate a triple $(\Span{i, j}, h, A)$ with each vertex in the tree, where
% We define the following important properties of this parse tree for each vertex:


\begin{itemize}
\item $\Span{i,j}$; the \textit{span}  of the vertex, i.e. the contiguous sequence $\{x_i, \ldots, x_j\}$ of the original input covered by the vertex.

\item $h \in \{0, \ldots n\}$; index indicating that $x_h$ is the \textit{head} of the vertex, defined recursively by the following rules:
  \begin{enumerate}
  \item  If the vertex is leaf $x_i$, then $h=i$.
  \item Otherwise,  $h$ is the head of the $k$'th child of the vertex where $\langle A \rightarrow \beta_1 \ldots \beta_m, k\rangle$  is the rule production at this vertex.
  \end{enumerate}
  Let $h(v)$ indicate the head of vertex $v$.
\item $A \in \terms \cup \nonterms$; the terminal or nonterminal symbol of the vertex.
\end{itemize}

In a parse, each word $x_i$ begins as a head at the fringe of the tree, and at some ancestor vertex ceases to play this role. To capture this notion, define the \textit{spine} of index $i$ to be the
longest chain of vertices $v_1 \ldots
v_p$, where $v_1$ is at the fringe and for all $j \in \{2, \ldots, p\}$, $v_j$ is
the parent of $v_{j-1}$ and $h(v_j) = i$.
Define the vertex $v_{p+1}$ to be the parent of the top vertex $v_p$, where $h(v_{p+1}) \neq i$.


% Finally for a given parse and any input index $i \in \{1 \ldots n\}$ define
% the \textit{spine} of that index as a chain of

% Define a lexicalized context-free grammar as a grammar G with a set of head rules that indicate which rhs position is the head of each rule. That is we

% For any grammar define its head rules as a function $H: \rules \mapsto \{0,\ldots \}$.
% For example for a rule $ N \rightarrow $.  For a deeper look at head rules set \cite{collin}


% Each vertex of the tree can be identified with a tuple $(\Span{i, j}, h, A)$ where $\Span{i, j}$ are the words covered by the vertex, $A$ is the syntactic label of the vertex, and $h$ is the head of the vertex.

% span $\Span{i, j}$ in the sentence and has a symbol $\terms$ and uses rule $r$.

\subsection{Binarization}

Any LCFG is weakly-equivalent to a binarized LCFG where all rules are either binary or unary. In this work we use the following binarization.

Consider a rule $\langle A \rightarrow \beta_1 \ldots \beta_m, k\rangle$. We instead introduce the following binarized context-free rules.

$\vdots$

Consider a rule $\langle A \rightarrow \beta_1 \ldots \beta_m, k\rangle$. We instead introduce the following binarized context-free rules.


\todo[inline]{ fix me}

\begin{itemize}
\item $\langle A  \rightarrow \beta_1\ A_{l, 1}, 2 \rangle $

\item
$\langle A_{l, i-1}  \rightarrow \beta_i\ A_{l, i}, 1 \rangle $ for $1< i < c$
\item
$\langle A_{r, i-1}  \rightarrow  A_{r, i} \ \beta_i, 1 \rangle $ for $i > c$

% \item
% if
% $\langle A_{} \rightarrow \beta_c\ A_{} \rangle $

% else
% $\langle  A_{} \rightarrow  A_{}\ \beta_c  \rangle $
% end
\end{itemize}


From here on we assume that all tree productions are binary. The benefit of using a binarized LCFG is that the set of tree $\mathcal{Y}(x)$ can be compactly enumerated using the inductive productions of lexicalized CKY. These production are shown in Figure~\ref{}. The main production used is

% For a given sentence, we can define the set of possible productions is defined by three items
\[ \infer{(\Span{i, j}, h, A)}{(\Span{i, k}, m, B) &  (\Span{k+1, j}, h, C)} \]
 for $A \in \nonterms, B, C \in \nonterms \cup \terms, i < k < j$. This rule indicates that rule $\langle A \rightarrow B\ C, 1\rangle$  was applied at a vertex covering $\Span{i, j}$ to produce two veritices covering $\Span{i, k}$ and $\Span{k+1, j}$.



We abbreviate this decision as a tuple $(\Span{i ,k, j}, h, m, r)$, and for $y \in \mathcal{Y}$, define  $y$ as an indicator vector over decisions i.e.  $(\Span{i ,k, j}, h, m, r) = 1$ to indicate that the rule above was used.


% Recall that the each vertex in the tree has the form $(\Span{i, j}, h,
% A)$. After binarization, each tree production is roughly of the form
% $(\Span{i, j}, h, A) \rightarrow (\Span{i, k}, m, B)\ (\Span{k+1, j}, h, C)$ for $A \in \nonterms, B, C \in \nonterms \cup \terms, i < k < j,$ and  .
% For notational simplicity, we write this production
% where $r = \langle A \rightarrow B\ C, 1\rangle $ is the rule applied.


% To begin we consider parsing a standard lexicalized context-free grammar with a
% factored scoring function.

The lexicalized parsing problem is to find the highest-scoring parse in this set, i.e.  \[ \hat{y} \gets \argmax_{y \in {\cal Y}} s(y;x) \] where
$s: \mathcal{Y} \rightarrow \Reals$ is a linear scoring function that scores each production.

This combinatorial optimization problem can be solved by bottom-up dynamic
programming over the set of rule productions shown in figure~\ref{}.
Since there are five free indices $i, j,  k, h, m$ and $|\rules|$ possible grammartical, in the worst-case this algorithm
 $O(n^5 \rules)$ running time.


% by using a simple lexicalized extension to the standard
% CKY parsing algorithm, shown in Figure~\ref{}.







% Given a sentence $x_1 \ldots x_n$, let ${\cal Y}$ be all possible valid parses
% under the binarized grammar. And let each parse $y\in {\cal Y}$  be given an a indicator
% vector of possible productions over where $y(\Span{i ,j , k}, h, m, r) = 1$ if the
% production appears in the parse and $0$ otherwise.



\subsection{Dependency Parsing}

Dependency trees provide an alternative, and in some sense simpler,
representation of grammatical structure.

Given an input sentence $x_1
\ldots x_n$, we allow each word to modify another word,  and define these dependency decisions as a sequence $d_1 \ldots d_n$ where for all $i$, $d_i \in \{0, \ldots, n\}$ and $0$ is a pseudo-root position. These dependency relations can be seen as arcs $(d_i, i)$ in a directed graph. (as
shown in Figure~\ref{}). A dependency parse is valid  if the corresponding directed graph is a directed tree rooted at
vertex $0$.
% to be the head of the sentence, i.e. it modifies $g_i \in \{0, n\}$
% where $0$ is a special pseudo-root symbol.
% If each word and the pseudo-root is represented as a vertex, than
% these relations correspond to arcs $(g_i, i)$ in a directed graph

Given a valid dependency parse, we can define the span of any word $m$ as the set of indices reachable from vertex $m$ in the directed tree. A dependency parse is \textit{projective} if the descendants of every word in the tree form a contiguous span of the original sentence \cite{}. For we use the notation $\Left{m}$ and $\Right{m}$ to represent the left- and
right-boundaries of this span.

The highest-scoring projective dependency parse under an arc-factored scoring function can be found in time $O(n^3)$.


\begin{figure}
  \centering


  \Tree [ .\node[color=red]{$(\Span{1,n}, 3, \Tag{S})$}; [ .\node[color=blue]{$(\Span{1,2}, 2,  \Tag{NP})$}; [  .\node{$(\Span{1,1}, 1,  \Tag{DT})$}; The$_1$ ]  [ .\node[color=blue]{$(\Span{2,2}, 2, \Tag{NN})$}; \textcolor{blue}{automaker$_2$} ] ] [ .\node{$(\Span{3,3}, 3,  \Tag{VBD})$}; sold$_3$ ] [ .\node{..}; ] ]

  % \Tree [ .A [ .B  C ] ]
  \caption{Figure illustrating a LCFG parse. The parse is an ordered tree with fringe $x_1, \ldots, x_n$. Each vertex is annotated with a span, head, and syntactic tag. The blue vertices represent the 3-vertex spine $v_1, v_2, v_3$ of the word \texttt{automaker$_2$}. The root vertex is $v_4$, which implies that \texttt{automaker$_2$} modifies \texttt{sold$_3$} in the induced dependency graph.     }
\end{figure}


% define a sequence of dependencies $()$

% define a sequence $g_1 \ldots
% g_n$ where for all $i$, $t_i$ is the dependent of position $g_i \in  \{0,
% \ldots n\}$, and $0$ is a special pseudo-root symbol.

We now give the important property for this work.

\begin{lemma}
Any lexicalized context-free parse, as defined above, can be converted
deterministically into a projective dependency tree.
\end{lemma}


Conversion.

For an input symbol $x_m$ with spine $v_1, \ldots, v_p$,

\begin{enumerate}
\item If $v_p$ is the root of the tree,
then $d_m = 0$.
\item Otherwise let $v_{p+1}$ be the parent vertex of
$v_p$ and $g_m = h(v_{p+1})$. The span $\Span{i, j}$ of $v_p$ in the lexicalized parse is equivalent to $\Span{\Left{m}, \Right{m}}$
in the induced dependency parse.
\end{enumerate}

% In the binarized case,


The conversion produces a directed tree rooted at $0$, by preserving the tree structure of the original LCFG parse.


\section{Overview}

In recent years there has been a steady progression in the speed and accuracy of dependency parsers \cite{}; however
there are still many tasks that rely heavily on the full context-free parses \cite{}. It is therefore desirable
to produce context-free parses directly from dependency structures.

However, while this conversion is deterministic from lexicalized context-free grammars
to dependency parses, the inverse problem is more difficult. For a given dependency
graph there may be many different valid context-free parses. This problem is the focus of this work.



This conversion is challenging in two ways:

\begin{itemize}
\item The original dependency parse does not contain any information about the symbols at each vertex. From the context we need to
determine the nonterminals $A$ and the rules used.

\item The dependency structure does not specify the shape of the context-free parse. Even without syntactic symbols many
context-free trees may be valid for a single dependency parse. This is shown in Figure~\ref{}
\end{itemize}


Because of these issues is not obvious how to directly perform this conversion without doing search over possible structures.




\begin{figure}
  \centering

  \Tree [ .X [ .X $x_1$ ]  [ .X $x_2$ ] ]
  \hspace{0.2cm}
  \Tree [ .X [ .X $x_1$ ]  [ .X $x_2$ ] ]
  \hspace{0.2cm}
  \Tree [ .X [ .X $x_1$ ]  [ .X $x_2$ ] ]
  \hspace{0.2cm}
  \Tree [ .X [ .X $x_1$ ]  [ .X $x_2$ ] ]

  \caption{While an LCFG parse determines a unique dependency parse, the inverse problem is non-deterministic. The figure show several LCFG parse structures that all correspond to the dependency structure shown. }
\end{figure}



% Any lexicalized







% For a specific parse $p$ the head rules induce a projective dependency parse over the sentence.
% To see this, as a base case associate a head word $i$ with each node $(i, i)$ in the tree.
% Then inductively at each inner node with span $(i,j)$ assign the head word $i \leq h \leq$ where
% $h$ is the head word of the $H(r)$th child node.

% The span of a word $i$ is the span $(i, j)$ of $p_n$.


% Define the spine of a word $i$ as the sequence of nodes $p_1, \ldots p_n$ such that $h(p_j) = i$.
% Let $p_{n+1}$ be the parent of $p_n$, then we say that $i$ is the dependent of $h(p_{n+1})$. By convention
% if $p_n$ is the root of the tree then $i$ is the dependent of the pseudo-root $0$.

% We can think of these dependencies as a directed graph over the nodes $\{0, \ldots n\}$ where the arc $(j, i)$ indicates that
% word $i$ is a dependent of word $j$.


% $l,r:\{1\ldots n\} \mapsto \{1\ldots n\}$


% While we have described dependencies as the deterministic by-product
% of phrase-structure parsing, in recent years there has been extensive
% work on directly predicting the dependency structure $(j,i)$ without
% predicting the full parse structure. The main benefit of this approach
% is that these structures can be predicted efficiently which has allowed
% for extensive use of powerful structured prediction techniques.






% A labeled dependency parse is one with a set $\mathcal{L}$
% and $(i, j, l)$.


% \subsection{Dependency Parsing}


% A dependency parse is any directed tree over this graph rooted
% at node $0$.


% If there exists a path from $w_i$ to $w_j$ than $w_j$ is a
% an ancestor of word $i$.

% Define a \textit{projective} dependency parse as one where for any word
% the ancestors are contiguous.

% We can specify a dependency parse as a sequence $(i,j)$.


\section{Parsing Dependencies}

To solve this inverse problem we set up as
simple variant of the original lexicalized parsing algorithm. While this algorithm has the same form of the original, we show that it can be solved asymptotically faster.

\subsection{Algorithm}

Recall that $\mathcal{Y}(x)$ is the set of valid LCFG parses for sentence $x$. We now assume that we now additionally are given a projective dependency parse $d_1, \ldots, d_n$. Define the set $\mathcal{Y}(x,d)$ as LCFG parses that induce this dependency structure, i.e.

\begin{eqnarray*}
  {\cal Y}(x,d ) & =& \{ y \in {\cal Y}: \\
&&\mathrm{for\ all\ } d_m =  h \mathrm{ \ there\ exists\ } i, j, k, r \\
&&\mathrm{s.t. \ }  y (i, j, k, h, m, r) = 1   \}
\end{eqnarray*}



% Our contribution to this problem will

% In this section we first introduce a binarization approach for lexicalized context free

% For any parse produced in the new grammar, we can convert it to a tree
% in the original grammar with the same induced dependency tree.

% \todo[inline]{check this}



% However doing it for
% all parse structures yields a $O(n^5)$ running time algorithm.


% Given an instance of the problem $x$ consisting of a sentence, part-of-speech tags,
% and a predicted dependency parse.

% Let the set $\mathcal{Y}(x)$ represent all possible parse trees for a sentence.
% Define the set $\mathcal{Y}(x)$ to contain all tree that agree with the dependency parse structure given.
% That is if rule $y \in \mathcal{Y}$ and  $(i, j, k, h, m) \in y$ then $(h, m) \in x$.

% Our goal will be to solve the optimization $\argmax_{y \in \mathcal{Y}} w^{\top} y$ for some any vector $w$

% The standard algorithm for binarized context-free grammar is known as the CKY algorithm. For review
% the inductive form of the CKY algorithm is shown in Figure~\ref{fig:cky}.


% \subsection{Parsing Dependencies}

and our aim is to find

\[ \argmax_{y \in \mathcal{Y}(x, d)} s(y; x, d)\]

We make the following simple observation about this new problem. For word $m$ with spine $v_1 \ldots v_p$ the span $\Span{i,j}$ of $v_p$ is equal to $\langle \Left{m},\Right{m}\rangle$. These spans can be computed directly from $d$.


% Now we make a simple extension to this algorithm. In the last section $x$ was just the sentence, now we assume that we also are given the dependency parse. Define ${\cal Y}(x)$ to the be the set of parse tree that agree with the dependency structure.


% That is if a rule in the parse $y \in \mathcal{Y}$ then the implied dependency is $(h, m)$ is in the dependency parse. The new problem is to solve

% \[ \argmax_{y \in {\cal Y}(x)} s(y;x) \]

Figure~\ref{} shows the new inductive rules.
Recall that $\Left{m}$ and $\Right{m}$ are the precomputed left- and right- boundary words of the cover of word $m$.

While the algorithm is virtually identical to the standard lexicalized CKY algorithm, we are now able to fix many of the free indices. In fact, since there are $n$ dependency links $(h, m)$ and $n$ indices $i$ the new algorithm has $O(n^2|\rules|)$ running time.


\begin{figure}
  \noindent \textbf{Premise:}
  \[(\Span{i, i}, i, A)\ \ \ \forall i \in \{1 \ldots n\}, A \in {\cal N}\]

  \noindent\textbf{Rules:}


   For $i\leq m \leq j < h \leq k$,  and  rule  $\langle A \rightarrow B\ C, 1\rangle$,
   \[\infer{( \Span{i, j},  h,  A)}{( \Span{i, k}, h, B)  &  ( \Span{k, j}, m, C) } \]

   For $i\leq h \leq j < m \leq k$, rule  $\langle A \rightarrow B\ C,  2\rangle$,
   \[\infer{(\Span{i, j},  h, A)}{(\Span{i, k}, m, B)  &  (\Span{k, j}, h, C) }  \]

\noindent \textbf{Goal:}\[ (\Span{1, n}, m, \mathrm{root}) \mathrm{\ for\ any }\ m\]
\label{fig:cky}
\caption{Standard CKY algorithm for LCFG parsing stated as inductive rules. Starting from the \textit{premise}, any valid application of \textit{rules} that leads to a \textit{goal} is a valid parse. Finding the optimal parse with dynamic programming is linear in the number of rules. For this algorithm there are $O(n^5|\rules|)$ rules where $n$ is the length of the sentence.}

\end{figure}

% For contrast here is an analysis of a variant of our current parsing
% algorithm. This analysis exploits the following idea. Define
% $l,r:\{1\ldots n\} \mapsto \{1\ldots n\}$ as functions that return the
% left and right boundary of the descendants of a head word. These
% functions can be pre-computed efficiently for a given dep tree.

% In this analysis we only take modifiers $m$ with spans $(l(m), r(m))$
% since we know that is a requirement. Here I use $\tri$ to indicate a standard CKY item.

\begin{figure}
  \noindent \textbf{Premise:}
  \[(\langle i,i \rangle, i, A)\ \ \ \forall i \in \{1 \ldots n\}, A \in {\cal N}\]

  \noindent\textbf{Rules:}

   % For all    $h$,  nonterminals $A \in \nonterms$,
   % \[ \infer{( \tri, h, A)}{(\Span{\Left{h}, \Right{h}}, h, A) } \]


   For all   $i < m, h = d_m$  and rule  $\langle A \rightarrow B\ C, 1 \rangle$,
  \[\infer{( \Span{ i, \Right{m} }, h, A)}{(\Span{i, \Left{m}-1}, h, B)  &  (\tri, m, C) } \]

  For all    $m < j, h = d_m$ and  rule  $\langle A \rightarrow B\ C, 2\rangle$,
  \[ \infer{( \Span{\Left{m}, j}, h, A)}{(\tri, m, B)  &  (\Span{\Right{m}+ 1, j}, h, C) } \]


% \forall  i, j, h, A \rightarrow B^* \\\\
%   \infer{(\Span{i, j}, h, A)}{(\Span{i, j}, h, B)}  \\\
% \\

\noindent \textbf{Goal:}\[ (\Span{1, n}, m, \Root) \mathrm{\ for\ any }\ m \mathrm{\ s.t. \ } d_m = 0  \]

\caption{The constrained CKY parsing algorithm for $\mathcal{Y}(x, d)$. The algorithm is nearly identical to Figure~\ref{} except that many of the free indices are now fixed to the dependency parse. Finding the optimal parse is now $O(n^2|\rules|)$}.
\end{figure}

% Here we assume there can


\subsection{Extension: Labels}

\todo[inline]{Finish this section}

Standard dependency parsers also predict labels from a set ${\cal L}$ on each dependency link.
In a labeled dependency parser a would be of the form $(i, j, l)$.

This label information can be used to encode further information about the parse structure. For instance
if we use the label set ${\cal L} = \nonterms \times \nonterms \times \nonterms$, encoding the binary rule decisions $A \rightarrow B\ C$.






% \subsection{Complexity Analysis}

% Since we know the dep tree, there are exactly $n$ $h \rightarrow m$ arcs. Therefore there are $O(n^2 |{\cal R}|)$ binary terms.


% The unary terms are more difficult to limit. Worst case it will have $O(n^3 |{\cal R}_1|)$ terms where ${\cal R}_1$ is the set of unary rules.
% In practice there will be $O(n \times \max_h({\#l_h \times  \#r_h}) \times |{\cal R}_1|)$ terms where $\#r_h$ and $\#l_h$ are the number of left and right modifiers
% of $h$ respectively.


\section{Structured Prediction}

We model this problem using a standard structured prediction setup.

The goal is to predict the best possible LCFG parse for sentence.
We define the scoring function $s$ as

\[s(y;x, d, \theta) =  \theta^{\top} f(x, d, y) \]

\noindex
where $\theta \in \Reals^d $ is a learned weight vector and $f(x, d, y)$ is a linear feature function
that maps parse production to feature vectors. In this section we discuss the feature function $f$ and estimating the parameter vector $\theta$.

\subsection{Features}

% We model this problem using  a standard structured prediction set-up.



% \[\hat{y} \gets \argmax_{y\in \mathcal{Y}(x)}  s(y; x) \]

% where the function $f$ is a feature function that maps possible parse structures to
%   a sparse feature vector in $\{0,1\}^d$. For efficiency we assume that $f$
% factors as a sum of parts

% \[ f(x, y) = \sum_{(i, j, k, h, m, r) \in y} f(\langle i,j,k,h,m,r\rangle,  x) \]


We experimented with several settings of the part features.
The final features are shown in  Figure~\ref{fig:features}.

The first set of features look at the rule structure $A \rightarrow B\ C$.
We have features on several combinations of rule and non-terminals.

The next set of features look at properties of the lexicalization.
These look at combinations of the head word and tags.

The final set of features looks at more structural properties.


\todo[inline]{bug lp about this.}
% We have features on several combinations of rule and non-terminals.


\cite{}
\cite{}
\cite{}



\begin{figure}
  \centering
  For a part $(i, j, k, h, m, A \rightarrow B\ C)$
  \begin{tabular}{ll}

  $A \rightarrow B\ C$\\
  unary, A B\ C\\
  A, B \\
  A, C \\

  A, h.tag, m.tag \\
  A, B, m.tag \\
  A, C, h.tag \\
  A, h.tag \\
  A, h.word \\
  unary, h.tag \\

  $A \rightarrow B\ C$, h.tag, m.tag \\
  $A \rightarrow B\ C$, h.word, m.tag \\
  $A \rightarrow B\ C$, h.word \\
  $A \rightarrow B\ C$, h.tag \\
  $A \rightarrow B\ C$, m.word \\
  $A \rightarrow B\ C$, m.tag \\

  shape\\

  \end{tabular}

  \label{fig:features}
  \caption{The features used in our experiments.}
\end{figure}

\subsection{Training}

We train the parameters $\theta$ using a variant of standard structured SVM training \cite{}.

We assume that we are given a set of gold-annotated parse examples: $( x^{1}, y^{1}), \ldots,  (x^{D}, y^{D})$. We also define $d^{(1)} \ldots d^{(D)}$ as the dependency structures induced from $y^{1} \ldots y^{D}$


Our aim to find parameters that satisfy the following regularized risk minimization

\[ \min_{\theta} \sum_{i = 1}^D \max\{0,  \ell( x^{i}, d^{i} , y^{i}, \theta) \} + \frac{\lambda}{2} ||\theta||^2_2 \]

\noindent and define

\[\ell(x, d, y, \theta) = s(y) + \max_{y' \in \mathcal{Y}(x, d)}\left(s(y')  + \Delta(y, y') \right) \]


\noindent where $\Delta$ is a problem specific cost-function that we assume is linear in either arguments.


We consider two extensions to the standard setup.

First is that at training time we do not have access to the true dependency structure $d$, but instead only predicted dependencies $\hat{d}$. Past work on parsing  \cite{} has shown that it can be important to train using these predicted structures.

However, using $\hat{d}$ may make it impossible to recover the gold LCFG parse $y$, e.g. $y \not \in \mathcal{Y}(x, \hat{d})$. To handle this issue, we also replace $y$ with the oracle parse $\bar{y}$ defined at the projection of $y$ onto $\mathcal{Y}(x, \hat{d})$

\[ \bar{y} \gets \argmin_{y' \in \mathcal{Y}(x, \hat{d})} \Delta(y', y) \]

If $\Delta$ is linear in its first argument, this projection can be found by running the same algorithm as in Figure~\ref{}.

We replace the loss with $\ell(x^{(i)}, \hat{d}^{(i)}, \bar{y}^{(i)})

% \[l(i, \theta) = s(\bar{y}^{(i)}) + \max_{y \in \mathcal{Y}(x^{(i)}, \hat{d}^{(i)})}\left( s(y)  + \Delta(\bar{y}^{(i)}, y) \right) \]

In our experiments, we use a simple hamming loss $ \Delta(y, \bar{y}) = || y - \bar{y}||$.



At training time, we run 10-fold jack-knifing to produce dependency parses $\hat{d}$. We then run a single pass to calculate $\bar{y}$ for each training example.

We then optimize using stochastic gradient descent \cite{}. The gradient requires calculating a loss-augmented argmax for each training example.


% One issue with directly minimizing this objective, is that

%  for this problem is the role that
% parsing issues play. If there is a dependency parsing mistake makes it often impossible
% for the system to recover the correct parse.


% Define a loss function $ \Delta(\hat{y}, y)$ to determine the difference between two parses.
% We assume that  the loss function is linear in its first argument.




% Define the oracle parse to be the parse minimizes the loss function


% Since this functions is linear we can use our

% To learn the weight vector $\theta$ we optimize a standard structured svm objective




% Note that we are optimizing towards the oracle parse.


\section{Data and Setup}


\subsection{Data}

We used wsj..

We used ctb 5-1..




\subsection{Implementation}

We built...

Parser is in C++, publicly available, 500 lines of code..


\subsection{Binarization}

Before describing our parsing algorithm we first describe a binarization approach to make efficient parsing possible and highlight the relationship between the LCFG and the dependency parse.

% We can binarize a context-free grammar to produce a new grammar where for all rules $A \rightarrow \beta$ the length of $\beta$ is in either $\{1, 2\}$
% in such a way that we can obtain the original tree.

% We first describe our binarization approach. Our aim is for every non-unary rule to correspond to exactly one dependency arc.


\subsection{Extension: Pruning}

We also experiment with a simple pruning dictionary pruning technique.
For each context-free rule $A \rightarrow B\ C$ and POS tag $a$ we
remove all rules that were not seen with that tag as a head in training.



\subsection{}

\section{Results}


\subsection{}


\begin{table*}
  \centering
  Parsing Results


  \begin{tabular}{l|lllllll}
    \hline
    & \multicolumn{7}{|c}{wsj} \\
    \hline
    & speed & fscore \\
    \hline
    Petrov & & & \\
    Carreras & & & \\
    \hline
    \hline
    & \multicolumn{7}{|c}{ctb} \\
    \hline
  \end{tabular}
  \caption{This is the big monster result table that should tower above all comers. }
\end{table*}

\begin{table}
  \centering
  \begin{tabular}{l|ll}
    Model & fscore & speed  \\
    \hline
    TurboParser & & \\
    MaltParser & & \\
    EasyFirst & & \\
  \end{tabular}
  \caption{This }
\end{table}

\textbf{Acknowledgment} sections should go as a last (unnumbered) section immediately
before the references.

% \bibliography{full}
% \bibliographystyle{naaclhlt2015}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End
